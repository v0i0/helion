This file is automatically generated by assertExpectedJournal calls in test_reductions.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestReductions.test_argmin_argmax)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _reduce_kernel_kernel(x, out, out_size_0, x_size_0, x_size_1, out_stride_0, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, 0], [_BLOCK_SIZE_0, _RDIM_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], load, float('-inf'))
    argmax = triton_helpers.max_with_index(_mask_to, tl.broadcast_to(indices_1[None, :], [_BLOCK_SIZE_0, _RDIM_SIZE_1]), 1)[1].to(tl.int64)
    tl.store(tl.make_block_ptr(out, [out_size_0], [out_stride_0], [offset_0], [_BLOCK_SIZE_0], [0]), argmax, boundary_check=[0])

def reduce_kernel(x: torch.Tensor, fn: Callable[[torch.Tensor], torch.Tensor], out_dtype=torch.float32, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty([n], dtype=out_dtype, device=x.device)
    _BLOCK_SIZE_0 = 16
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_reduce_kernel_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.size(0), x.size(0), x.size(1), out.stride(0), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestReductions.test_argmin_argmax_looped)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _reduce_kernel_kernel(x, out, out_size_0, x_size_0, x_size_1, out_stride_0, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    argmax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    argmax_acc_index = tl.full([1, _REDUCTION_BLOCK_1], 2147483647, tl.int32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, roffset_1], [1, _REDUCTION_BLOCK_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, float('-inf'))
        argmax_acc, argmax_acc_index = triton_helpers.maximum_with_index(argmax_acc, argmax_acc_index, _mask_to, tl.broadcast_to(rindex_1[None, :], [1, _REDUCTION_BLOCK_1]))
    argmax = triton_helpers.max_with_index(argmax_acc, argmax_acc_index, 1)[1].to(tl.int64)
    tl.store(tl.make_block_ptr(out, [out_size_0], [out_stride_0], [offset_0], [1], [0]), argmax, boundary_check=[0])

def reduce_kernel(x: torch.Tensor, fn: Callable[[torch.Tensor], torch.Tensor], out_dtype=torch.float32, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty([n], dtype=out_dtype, device=x.device)
    _REDUCTION_BLOCK_1 = 16
    _launcher(_reduce_kernel_kernel, (n,), x, out, out.size(0), x.size(0), x.size(1), out.stride(0), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestReductions.test_fp16_var_mean)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _layer_norm_fwd_repro_kernel(x, weight, bias, out, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    x_part = tl.load(x + (indices_0[:, None] * 64 + indices_1[None, :] * 1), None)
    v_0 = x_part.to(tl.float32)
    var_mean_extra = tl.reshape(tl.sum(v_0, 1), [_BLOCK_SIZE_0, 1])
    v_1 = 64
    v_2 = var_mean_extra / v_1.to(tl.float32)
    v_3 = x_part.to(tl.float32)
    v_4 = v_3 - v_2
    v_5 = v_4 * v_4
    var_mean_extra_2 = tl.reshape(tl.sum(v_5, 1), [_BLOCK_SIZE_0, 1])
    v_6 = 64
    v_7 = var_mean_extra_2 / v_6.to(tl.float32)
    v_8 = v_7.to(tl.float16)
    v_9 = v_2.to(tl.float16)
    v_10 = x_part - v_9
    v_11 = v_8.to(tl.float32)
    v_12 = v_11 + eps
    v_13 = libdevice.rsqrt(v_12)
    v_14 = v_10.to(tl.float32)
    v_15 = v_14 * v_13
    load_1 = tl.load(weight + indices_1 * 1, None)
    v_16 = load_1.to(tl.float32)
    v_17 = v_16[None, :]
    v_18 = v_15 * v_17
    load_2 = tl.load(bias + indices_1 * 1, None)
    v_19 = load_2.to(tl.float32)
    v_20 = v_19[None, :]
    v_21 = v_18 + v_20
    v_22 = v_21.to(tl.float16)
    tl.store(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), v_22, None)

def layer_norm_fwd_repro(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    m, n = x.size()
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    _launcher(_layer_norm_fwd_repro_kernel, (triton.cdiv(32, _BLOCK_SIZE_0),), x, weight, bias, out, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestReductions.test_fp16_var_mean)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _layer_norm_fwd_repro_kernel(x, weight, bias, out, eps, _BLOCK_SIZE_0: tl.constexpr, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    var_mean_extra_acc = tl.full([_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, 64, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        x_part = tl.load(x + (indices_0[:, None] * 64 + rindex_1[None, :] * 1), None)
        v_0 = x_part.to(tl.float32)
        v_1 = var_mean_extra_acc + v_0
        var_mean_extra_acc = v_1
    var_mean_extra = tl.reshape(tl.sum(var_mean_extra_acc, 1), [_BLOCK_SIZE_0, 1])
    v_2 = 64
    v_3 = var_mean_extra / v_2.to(tl.float32)
    var_mean_extra_2_acc = tl.full([_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, 64, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        v_3_copy = v_3
        x_part_1 = tl.load(x + (indices_0[:, None] * 64 + rindex_1[None, :] * 1), None)
        v_4 = x_part_1.to(tl.float32)
        v_5 = v_4 - v_3_copy
        v_6 = v_5 * v_5
        v_7 = var_mean_extra_2_acc + v_6
        var_mean_extra_2_acc = v_7
    var_mean_extra_2 = tl.reshape(tl.sum(var_mean_extra_2_acc, 1), [_BLOCK_SIZE_0, 1])
    v_8 = 64
    v_9 = var_mean_extra_2 / v_8.to(tl.float32)
    v_10 = v_9.to(tl.float16)
    v_11 = v_3.to(tl.float16)
    v_12 = v_10.to(tl.float32)
    v_13 = v_12 + eps
    v_14 = libdevice.rsqrt(v_13)
    for roffset_1 in tl.range(0, 64, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        v_11_copy = v_11
        v_14_copy = v_14
        x_part_2 = tl.load(x + (indices_0[:, None] * 64 + rindex_1[None, :] * 1), None)
        v_15 = x_part_2 - v_11_copy
        v_16 = v_15.to(tl.float32)
        v_17 = v_16 * v_14_copy
        load_1 = tl.load(weight + rindex_1 * 1, None)
        v_18 = load_1.to(tl.float32)
        v_19 = v_18[None, :]
        v_20 = v_17 * v_19
        load_2 = tl.load(bias + rindex_1 * 1, None)
        v_21 = load_2.to(tl.float32)
        v_22 = v_21[None, :]
        v_23 = v_20 + v_22
        v_24 = v_23.to(tl.float16)
        tl.store(out + (indices_0[:, None] * 64 + rindex_1[None, :] * 1), v_24, None)

def layer_norm_fwd_repro(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    m, n = x.size()
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 32
    _REDUCTION_BLOCK_1 = 8
    _launcher(_layer_norm_fwd_repro_kernel, (triton.cdiv(32, _BLOCK_SIZE_0),), x, weight, bias, out, eps, _BLOCK_SIZE_0, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestReductions.test_mean)
def reduce_kernel(x: torch.Tensor, fn: Callable[[torch.Tensor], torch.Tensor], out_dtype=torch.float32):
    # Call: SequenceType((SymIntType(s77), SymIntType(s27))) SourceOrigin(location=<SourceLocation test_reductions.py:48>)
    # Attribute: TensorAttributeType AttributeOrigin(value=ArgumentOrigin(name='x'), key='size')
    # Name: TensorType([x_size0, x_size1], torch.float32) ArgumentOrigin(name='x')
    n, _m = x.size()
    # Call: TensorType([x_size0], torch.float32) SourceOrigin(location=<SourceLocation test_reductions.py:49>)
    # Attribute: CallableType(_VariableFunctionsClass.empty) AttributeOrigin(value=GlobalOrigin(name='torch'), key='empty')
    # Name: PythonModuleType(torch) GlobalOrigin(name='torch')
    # List: SequenceType([SymIntType(s77)]) SourceOrigin(location=<SourceLocation test_reductions.py:50>)
    # Name: SymIntType(s77) GetItemOrigin(value=SourceOrigin(location=<SourceLocation test_reductions.py:48>), key=0)
    # Name: LiteralType(torch.float32) ArgumentOrigin(name='out_dtype')
    # Attribute: LiteralType(device(type='cuda', index=0)) AttributeOrigin(value=ArgumentOrigin(name='x'), key='device')
    # Name: TensorType([x_size0, x_size1], torch.float32) ArgumentOrigin(name='x')
    # For: loop_type=GRID
    out = torch.empty([n], dtype=out_dtype, device=x.device)
    # Call: IterType(TileIndexType(0)) SourceOrigin(location=<SourceLocation test_reductions.py:54>)
    # Attribute: CallableType(tile) AttributeOrigin(value=GlobalOrigin(name='hl'), key='tile')
    # Name: PythonModuleType(helion.language) GlobalOrigin(name='hl')
    # Name: SymIntType(s77) GetItemOrigin(value=SourceOrigin(location=<SourceLocation test_reductions.py:48>), key=0)
    for tile_n in hl.tile(n):
        # Subscript: TensorType([block_size_0], torch.float32) DeviceOrigin(location=<SourceLocation test_reductions.py:55>)
        # Name: TensorType([x_size0], torch.float32) SourceOrigin(location=<SourceLocation test_reductions.py:49>)
        # Name: TileIndexType(0) SourceOrigin(location=<SourceLocation test_reductions.py:54>)
        # Call: TensorType([block_size_0], torch.float32) DeviceOrigin(location=<SourceLocation test_reductions.py:55>)
        # Name: CallableType(_VariableFunctionsClass.mean) ArgumentOrigin(name='fn')
        # Subscript: TensorType([block_size_0, rdim_1], torch.float32) DeviceOrigin(location=<SourceLocation test_reductions.py:55>)
        # Name: TensorType([x_size0, x_size1], torch.float32) ArgumentOrigin(name='x')
        # Name: TileIndexType(0) SourceOrigin(location=<SourceLocation test_reductions.py:54>)
        # Slice: SliceType(LiteralType(None):LiteralType(None):LiteralType(None)) DeviceOrigin(location=<SourceLocation test_reductions.py:55>)
        # UnaryOp: LiteralType(-1) DeviceOrigin(location=<SourceLocation test_reductions.py:55>)
        # Constant: LiteralType(1) DeviceOrigin(location=<SourceLocation test_reductions.py:55>)
        out[tile_n] = fn(x[tile_n, :], dim=-1)
    # Name: TensorType([x_size0], torch.float32) SourceOrigin(location=<SourceLocation test_reductions.py:49>)
    return out

def root_graph_0():
    # File: .../test_reductions.py:55 in reduce_kernel, code: out[tile_n] = fn(x[tile_n, :], dim=-1)
    x: "f32[s77, s27]" = helion_language__tracing_ops__host_tensor('x')
    block_size_0: "Sym(u0)" = helion_language__tracing_ops__get_symnode('block_size_0')
    load: "f32[u0, u1]" = helion_language_memory_ops_load(x, [block_size_0, slice(None, None, None)], None);  x = None
    mean_extra: "f32[u0]" = helion_language__tracing_ops__inductor_lowering_extra([load]);  load = None
    mean: "f32[u0]" = torch.ops.aten.mean.dim(None, [-1], _extra_args = [mean_extra]);  mean_extra = None
    out: "f32[s77]" = helion_language__tracing_ops__host_tensor('out')
    store = helion_language_memory_ops_store(out, [block_size_0], mean, None);  out = block_size_0 = mean = store = None
    return None

def reduction_loop_1():
    # File: .../test_reductions.py:55 in reduce_kernel, code: out[tile_n] = fn(x[tile_n, :], dim=-1)
    x: "f32[s77, s27]" = helion_language__tracing_ops__host_tensor('x')
    block_size_0: "Sym(u0)" = helion_language__tracing_ops__get_symnode('block_size_0')
    load: "f32[u0, u1]" = helion_language_memory_ops_load(x, [block_size_0, slice(None, None, None)], None);  x = block_size_0 = None
    mean_extra: "f32[u0]" = helion_language__tracing_ops__inductor_lowering_extra([load]);  load = None
    return [mean_extra]

def root_graph_2():
    # File: .../test_reductions.py:55 in reduce_kernel, code: out[tile_n] = fn(x[tile_n, :], dim=-1)
    block_size_0: "Sym(u0)" = helion_language__tracing_ops__get_symnode('block_size_0')
    _get_symnode: "Sym(s27)" = helion_language__tracing_ops__get_symnode('rdim1')
    _for_loop = helion_language__tracing_ops__for_loop(1, [0], [_get_symnode], []);  _get_symnode = None
    getitem: "f32[u0]" = _for_loop[0];  _for_loop = None
    mean: "f32[u0]" = torch.ops.aten.mean.dim(None, [-1], _extra_args = [getitem]);  getitem = None
    out: "f32[s77]" = helion_language__tracing_ops__host_tensor('out')
    store = helion_language_memory_ops_store(out, [block_size_0], mean, None);  out = block_size_0 = mean = store = None
    return None

--- assertExpectedJournal(TestReductions.test_mean)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _reduce_kernel_kernel(x, out, out_size_0, x_size_0, x_size_1, out_stride_0, x_stride_0, x_stride_1, _m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    load = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, 0], [_BLOCK_SIZE_0, _RDIM_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    mean_extra = tl.sum(load, 1)
    v_0 = mean_extra / _m.to(tl.float32)
    tl.store(tl.make_block_ptr(out, [out_size_0], [out_stride_0], [offset_0], [_BLOCK_SIZE_0], [0]), v_0, boundary_check=[0])

def reduce_kernel(x: torch.Tensor, fn: Callable[[torch.Tensor], torch.Tensor], out_dtype=torch.float32, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty([n], dtype=out_dtype, device=x.device)
    _BLOCK_SIZE_0 = 8
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_reduce_kernel_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.size(0), x.size(0), x.size(1), out.stride(0), x.stride(0), x.stride(1), _m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestReductions.test_reduction_loops_integer_values)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _layer_norm_reduction_kernel(bias, x, weight, out, bias_size_0, bias_stride_0, out_stride_0, out_stride_1, weight_stride_0, x_stride_0, x_stride_1, m, eps, _BLOCK_SIZE_0: tl.constexpr, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < m
    var_mean_extra_acc = tl.full([_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, bias_size_0, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < bias_size_0
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
        v_0 = load.to(tl.float32)
        v_1 = var_mean_extra_acc + v_0
        var_mean_extra_acc = v_1
    var_mean_extra = tl.reshape(tl.sum(var_mean_extra_acc, 1), [_BLOCK_SIZE_0, 1])
    v_2 = var_mean_extra / bias_size_0.to(tl.float32)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_0[:, None], [_BLOCK_SIZE_0, 1]), v_2, 0)
    var_mean_extra_2_acc = tl.full([_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, bias_size_0, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < bias_size_0
        _mask_to_1_copy = _mask_to_1
        load_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
        v_3 = load_1.to(tl.float32)
        v_4 = v_3 - _mask_to_1_copy
        v_5 = v_4 * v_4
        v_6 = var_mean_extra_2_acc + v_5
        var_mean_extra_2_acc = v_6
    var_mean_extra_2 = tl.reshape(tl.sum(var_mean_extra_2_acc, 1), [_BLOCK_SIZE_0, 1])
    v_7 = var_mean_extra_2 / bias_size_0.to(tl.float32)
    v_8 = v_7 + eps
    v_9 = libdevice.rsqrt(v_8)
    for roffset_1 in tl.range(0, bias_size_0, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < bias_size_0
        v_2_copy = v_2
        v_9_copy = v_9
        load_2 = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
        v_10 = load_2.to(tl.float32)
        v_11 = v_10 - v_2_copy
        v_12 = v_11 * v_9_copy
        load_3 = tl.load(weight + rindex_1 * weight_stride_0, mask_1, other=0)
        v_13 = load_3.to(tl.float32)
        v_14 = v_13[None, :]
        v_15 = v_12 * v_14
        load_4 = tl.load(bias + rindex_1 * bias_stride_0, mask_1, other=0)
        v_16 = load_4.to(tl.float32)
        v_17 = v_16[None, :]
        v_18 = v_15 + v_17
        v_19 = v_18.to(tl.float16)
        tl.store(out + (indices_0[:, None] * out_stride_0 + rindex_1[None, :] * out_stride_1), v_19, mask_0[:, None] & mask_1[None, :])

def layer_norm_reduction(x: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    m, n = x.size()
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 32
    _REDUCTION_BLOCK_1 = 4
    _launcher(_layer_norm_reduction_kernel, (triton.cdiv(m, _BLOCK_SIZE_0),), bias, x, weight, out, bias.size(0), bias.stride(0), out.stride(0), out.stride(1), weight.stride(0), x.stride(0), x.stride(1), m, eps, _BLOCK_SIZE_0, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestReductions.test_sum)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _sum_kernel_kernel(x, out, out_stride_0, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    sum_1 = tl.sum(load, 1)
    tl.store(out + indices_0 * out_stride_0, sum_1, None)

def sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty([n], dtype=x.dtype, device=x.device)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_sum_kernel_kernel, (n,), x, out, out.stride(0), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestReductions.test_sum_keepdims)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _sum_kernel_keepdims_kernel(x, out, out_size_1, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    load = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [0, offset_0], [_RDIM_SIZE_1, _BLOCK_SIZE_0], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    sum_1 = tl.reshape(tl.sum(load, 0), [1, _BLOCK_SIZE_0])
    tl.store(tl.make_block_ptr(out, [1, out_size_1], [out_stride_0, out_stride_1], [0, offset_0], [1, _BLOCK_SIZE_0], [1, 0]), sum_1, boundary_check=[1])

def sum_kernel_keepdims(x: torch.Tensor, *, _launcher=_default_launcher):
    _n, m = x.size()
    out = torch.empty([1, m], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 16
    _RDIM_SIZE_1 = triton.next_power_of_2(_n)
    _launcher(_sum_kernel_keepdims_kernel, (triton.cdiv(m, _BLOCK_SIZE_0),), x, out, out.size(1), x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestReductions.test_sum_looped)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _sum_kernel_kernel(x, out, out_stride_0, x_stride_0, x_stride_1, n, _m, _BLOCK_SIZE_0: tl.constexpr, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < n
    sum_1_acc = tl.full([_BLOCK_SIZE_0, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
        v_0 = sum_1_acc + load
        sum_1_acc = v_0
    sum_1 = tl.sum(sum_1_acc, 1)
    tl.store(out + indices_0 * out_stride_0, sum_1, mask_0)

def sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty([n], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 2
    _REDUCTION_BLOCK_1 = 64
    _launcher(_sum_kernel_kernel, (triton.cdiv(n, _BLOCK_SIZE_0),), x, out, out.stride(0), x.stride(0), x.stride(1), n, _m, _BLOCK_SIZE_0, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out
