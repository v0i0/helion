This file is automatically generated by assertExpectedJournal calls in test_examples.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestExamples.test_add)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_add(x, y, out, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, y_stride_0, y_stride_1, _BLOCK_SIZE_0_1: tl.constexpr):
    offsets_0_1 = tl.program_id(0) * _BLOCK_SIZE_0_1 + tl.arange(0, _BLOCK_SIZE_0_1).to(tl.int32)
    indices_1 = offsets_0_1 % x_size_1
    indices_0 = offsets_0_1 // x_size_1
    mask_0_1 = offsets_0_1 < x_size_0 * x_size_1
    load = tl.load(x + (indices_0 * x_stride_0 + indices_1 * x_stride_1), mask_0_1, other=0)
    load_1 = tl.load(y + (indices_0 * y_stride_0 + indices_1 * y_stride_1), mask_0_1, other=0)
    v_0 = tl.cast(load_1, tl.float32)
    v_1 = load + v_0
    tl.store(out + (indices_0 * out_stride_0 + indices_1 * out_stride_1), v_1, mask_0_1)

def add(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """
    Add two tensors element-wise with broadcasting support.

    Args:
        x: First input tensor
        y: Second input tensor

    Returns:
        A new tensor containing the element-wise sum of x and y
    """
    x, y = torch.broadcast_tensors(x, y)
    out = torch.empty(x.shape, dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _BLOCK_SIZE_0_1 = 128
    _launcher(_helion_add, (triton.cdiv(x.size(0) * x.size(1), _BLOCK_SIZE_0_1), 1, 1), x, y, out, x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), y.stride(0), y.stride(1), _BLOCK_SIZE_0_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_attention_block_pointer)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_attention(q_view, k_view, v_view, out, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = 64
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
    acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    q = tl.load(tl.make_block_ptr(q_view, [64, 1024, 64], [65536, 64, 1], [offset_0, offset_1, 0], [1, _BLOCK_SIZE_1, 64], [2, 1, 0]), boundary_check=[0, 1, 2], padding_option='zero')
    for offset_2 in tl.range(0, 512, _BLOCK_SIZE_3):
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        k = tl.load(tl.make_block_ptr(k_view, [64, 64, 512], [32768, 1, 64], [offset_0, 0, offset_2], [1, 64, _BLOCK_SIZE_3], [2, 0, 1]), boundary_check=[0, 1, 2], padding_option='zero')
        qk = tl.reshape(tl.dot(tl.reshape(tl.cast(q_copy_0, tl.float16), [_BLOCK_SIZE_1, 64]), tl.reshape(tl.cast(k, tl.float16), [64, _BLOCK_SIZE_3]), input_precision='tf32', out_dtype=tl.float32), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        amax = tl.cast(tl.max(qk, 2), tl.float16)
        v_0 = 0.18033688
        v_1 = amax * v_0
        v_2 = tl.cast(v_1, tl.float32)
        v_3 = triton_helpers.maximum(m_i_copy_0, v_2)
        v_4 = 0.18033688
        v_5 = qk * v_4
        subscript = v_3[:, :, None]
        v_6 = tl.cast(v_5, tl.float32)
        v_7 = v_6 - subscript
        v_8 = libdevice.exp2(v_7)
        l_ij = tl.cast(tl.sum(v_8, 2), tl.float32)
        v_9 = m_i_copy_0 - v_3
        v_10 = libdevice.exp2(v_9)
        v_11 = l_i_copy_0 * v_10
        l_i = v_11 + l_ij
        subscript_1 = v_10[:, :, None]
        v_13 = acc_copy_0 * subscript_1
        v = tl.load(tl.make_block_ptr(v_view, [64, 512, 64], [32768, 64, 1], [offset_0, offset_2, 0], [1, _BLOCK_SIZE_3, 64], [2, 1, 0]), boundary_check=[0, 1, 2], padding_option='zero')
        v_14 = tl.cast(v_8, tl.float16)
        acc = tl.reshape(tl.dot(tl.reshape(tl.cast(v_14, tl.float16), [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(tl.cast(v, tl.float16), [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_13, [_BLOCK_SIZE_1, 64]), input_precision='tf32', out_dtype=tl.float32), [1, _BLOCK_SIZE_1, 64])
        m_i = v_3
    subscript_2 = l_i[:, :, None]
    v_15 = acc / subscript_2
    v_16 = tl.cast(v_15, tl.float16)
    tl.store(tl.make_block_ptr(out, [64, 1024, 64], [65536, 64, 1], [offset_0, offset_1, 0], [1, _BLOCK_SIZE_1, 64], [2, 1, 0]), v_16, boundary_check=[0, 1, 2])

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention.

    Implements the attention mechanism: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V

    Args:
        q_in: Query tensor of shape [..., seq_len_q, head_dim]
        k_in: Key tensor of shape [..., seq_len_k, head_dim]
        v_in: Value tensor of shape [..., seq_len_k, head_dim]

    Returns:
        Output tensor of shape [..., seq_len_q, head_dim]
    """
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _BLOCK_SIZE_1 = 128
    _BLOCK_SIZE_3 = 64
    _launcher(_helion_attention, (64 * triton.cdiv(1024, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

--- assertExpectedJournal(TestExamples.test_attention_dynamic)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_attention(q_view, k_view, v_view, out, q_in_size_1, k_view_stride_0, k_view_stride_1, k_view_stride_2, out_stride_0, out_stride_1, out_stride_2, q_view_stride_0, q_view_stride_1, q_view_stride_2, v_view_stride_0, v_view_stride_1, v_view_stride_2, m_dim, n_dim, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = q_in_size_1
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < m_dim
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
    acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    q = tl.load(q_view + (indices_0[:, None, None] * q_view_stride_0 + indices_1[None, :, None] * q_view_stride_1 + indices_4[None, None, :] * q_view_stride_2), mask_1[None, :, None], other=0)
    for offset_2 in tl.range(0, n_dim.to(tl.int32), _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        mask_3 = indices_2 < n_dim
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        k = tl.load(k_view + (indices_0[:, None, None] * k_view_stride_0 + indices_4[None, :, None] * k_view_stride_1 + indices_2[None, None, :] * k_view_stride_2), mask_3[None, None, :], other=0)
        qk = tl.reshape(tl.dot(tl.reshape(tl.cast(q_copy_0, tl.float32), [_BLOCK_SIZE_1, 64]), tl.reshape(tl.cast(k, tl.float32), [64, _BLOCK_SIZE_3]), input_precision='tf32', out_dtype=tl.float32), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        _mask_to_2 = tl.where(tl.broadcast_to(mask_1[None, :, None] & mask_3[None, None, :], [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3]), qk, tl.full([], float('-inf'), tl.float32))
        amax = tl.cast(tl.max(_mask_to_2, 2), tl.float32)
        v_0 = 0.18033688
        v_1 = amax * v_0
        v_2 = triton_helpers.maximum(m_i_copy_0, v_1)
        v_3 = 0.18033688
        v_4 = qk * v_3
        subscript = v_2[:, :, None]
        v_5 = v_4 - subscript
        v_6 = libdevice.exp2(v_5)
        _mask_to_3 = tl.where(tl.broadcast_to(mask_1[None, :, None] & mask_3[None, None, :], [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3]), v_6, tl.full([], 0, tl.float32))
        l_ij = tl.cast(tl.sum(_mask_to_3, 2), tl.float32)
        v_7 = m_i_copy_0 - v_2
        v_8 = libdevice.exp2(v_7)
        v_9 = l_i_copy_0 * v_8
        l_i = v_9 + l_ij
        subscript_1 = v_8[:, :, None]
        v_11 = acc_copy_0 * subscript_1
        v = tl.load(v_view + (indices_0[:, None, None] * v_view_stride_0 + indices_2[None, :, None] * v_view_stride_1 + indices_4[None, None, :] * v_view_stride_2), mask_3[None, :, None], other=0)
        acc = tl.reshape(tl.dot(tl.reshape(tl.cast(_mask_to_3, tl.float32), [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(tl.cast(v, tl.float32), [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_11, [_BLOCK_SIZE_1, 64]), input_precision='tf32', out_dtype=tl.float32), [1, _BLOCK_SIZE_1, 64])
        m_i = v_2
    subscript_2 = l_i[:, :, None]
    v_12 = acc / subscript_2
    tl.store(out + (indices_0[:, None, None] * out_stride_0 + indices_1[None, :, None] * out_stride_1 + indices_4[None, None, :] * out_stride_2), v_12, mask_1[None, :, None])

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention.

    Implements the attention mechanism: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V

    Args:
        q_in: Query tensor of shape [..., seq_len_q, head_dim]
        k_in: Key tensor of shape [..., seq_len_k, head_dim]
        v_in: Value tensor of shape [..., seq_len_k, head_dim]

    Returns:
        Output tensor of shape [..., seq_len_q, head_dim]
    """
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _BLOCK_SIZE_1 = 32
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_3 = 32
    _launcher(_helion_attention, (q_in.size(1) * triton.cdiv(m_dim, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, q_in.size(1), k_view.stride(0), k_view.stride(1), k_view.stride(2), out.stride(0), out.stride(1), out.stride(2), q_view.stride(0), q_view.stride(1), q_view.stride(2), v_view.stride(0), v_view.stride(1), v_view.stride(2), m_dim, n_dim, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

--- assertExpectedJournal(TestExamples.test_attention_persistent_interleaved_l2_grouping)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_attention(q_view, k_view, v_view, out, _NUM_SM: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    total_pids = 32 * tl.cdiv(512, _BLOCK_SIZE_1)
    for virtual_pid in tl.range(tl.program_id(0), total_pids, _NUM_SM):
        num_pid_m = 32
        num_pid_n = tl.cdiv(512, _BLOCK_SIZE_1)
        inner_2d_pid = virtual_pid
        num_pid_in_group = 4 * num_pid_n
        group_id = inner_2d_pid // num_pid_in_group
        first_pid_m = group_id * 4
        group_size_m = min(num_pid_m - first_pid_m, 4)
        pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
        pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
        offset_0 = pid_0
        offset_1 = pid_1 * _BLOCK_SIZE_1
        m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
        l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
        acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
        q = tl.load(tl.make_block_ptr(q_view, [32, 512, 64], [32768, 64, 1], [offset_0, offset_1, 0], [1, _BLOCK_SIZE_1, 64], [2, 1, 0]), boundary_check=[0, 1, 2], padding_option='zero')
        for offset_2 in tl.range(0, 512, _BLOCK_SIZE_3):
            q_copy = q
            m_i_copy = m_i
            l_i_copy = l_i
            acc_copy = acc
            q_copy_0 = q_copy
            m_i_copy_0 = m_i_copy
            l_i_copy_0 = l_i_copy
            acc_copy_0 = acc_copy
            k = tl.load(tl.make_block_ptr(k_view, [32, 64, 512], [32768, 1, 64], [offset_0, 0, offset_2], [1, 64, _BLOCK_SIZE_3], [2, 0, 1]), boundary_check=[0, 1, 2], padding_option='zero')
            qk = tl.reshape(tl.dot(tl.reshape(tl.cast(q_copy_0, tl.float16), [_BLOCK_SIZE_1, 64]), tl.reshape(tl.cast(k, tl.float16), [64, _BLOCK_SIZE_3]), input_precision='tf32', out_dtype=tl.float32), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
            amax = tl.cast(tl.max(qk, 2), tl.float16)
            v_0 = 0.18033688
            v_1 = amax * v_0
            v_2 = tl.cast(v_1, tl.float32)
            v_3 = triton_helpers.maximum(m_i_copy_0, v_2)
            v_4 = 0.18033688
            v_5 = qk * v_4
            subscript = v_3[:, :, None]
            v_6 = tl.cast(v_5, tl.float32)
            v_7 = v_6 - subscript
            v_8 = libdevice.exp2(v_7)
            l_ij = tl.cast(tl.sum(v_8, 2), tl.float32)
            v_9 = m_i_copy_0 - v_3
            v_10 = libdevice.exp2(v_9)
            v_11 = l_i_copy_0 * v_10
            l_i = v_11 + l_ij
            subscript_1 = v_10[:, :, None]
            v_13 = acc_copy_0 * subscript_1
            v = tl.load(tl.make_block_ptr(v_view, [32, 512, 64], [32768, 64, 1], [offset_0, offset_2, 0], [1, _BLOCK_SIZE_3, 64], [2, 1, 0]), boundary_check=[0, 1, 2], padding_option='zero')
            v_14 = tl.cast(v_8, tl.float16)
            acc = tl.reshape(tl.dot(tl.reshape(tl.cast(v_14, tl.float16), [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(tl.cast(v, tl.float16), [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_13, [_BLOCK_SIZE_1, 64]), input_precision='tf32', out_dtype=tl.float32), [1, _BLOCK_SIZE_1, 64])
            m_i = v_3
        subscript_2 = l_i[:, :, None]
        v_15 = acc / subscript_2
        v_16 = tl.cast(v_15, tl.float16)
        tl.store(tl.make_block_ptr(out, [32, 512, 64], [32768, 64, 1], [offset_0, offset_1, 0], [1, _BLOCK_SIZE_1, 64], [2, 1, 0]), v_16, boundary_check=[0, 1, 2])

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention.

    Implements the attention mechanism: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V

    Args:
        q_in: Query tensor of shape [..., seq_len_q, head_dim]
        k_in: Key tensor of shape [..., seq_len_k, head_dim]
        v_in: Value tensor of shape [..., seq_len_k, head_dim]

    Returns:
        Output tensor of shape [..., seq_len_q, head_dim]
    """
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _NUM_SM = helion.runtime.get_num_sm(q_in.device)
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_3 = 64
    _launcher(_helion_attention, (_NUM_SM,), q_view, k_view, v_view, out, _NUM_SM, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

--- assertExpectedJournal(TestExamples.test_attention_pointer)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_attention(q_view, k_view, v_view, out, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = 32
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_4 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    m_i = tl.full([1, _BLOCK_SIZE_1], float('-inf'), tl.float32)
    l_i = tl.full([1, _BLOCK_SIZE_1], 1.0, tl.float32)
    acc = tl.full([1, _BLOCK_SIZE_1, 64], 0.0, tl.float32)
    q = tl.load(q_view + (indices_0[:, None, None] * 32768 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
    for offset_2 in tl.range(0, 512, _BLOCK_SIZE_3):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        q_copy = q
        m_i_copy = m_i
        l_i_copy = l_i
        acc_copy = acc
        q_copy_0 = q_copy
        m_i_copy_0 = m_i_copy
        l_i_copy_0 = l_i_copy
        acc_copy_0 = acc_copy
        k = tl.load(k_view + (indices_0[:, None, None] * 32768 + indices_4[None, :, None] * 1 + indices_2[None, None, :] * 64), None)
        qk = tl.reshape(tl.dot(tl.reshape(tl.cast(q_copy_0, tl.float32), [_BLOCK_SIZE_1, 64]), tl.reshape(tl.cast(k, tl.float32), [64, _BLOCK_SIZE_3]), input_precision='tf32', out_dtype=tl.float32), [1, _BLOCK_SIZE_1, _BLOCK_SIZE_3])
        amax = tl.cast(tl.max(qk, 2), tl.float32)
        v_0 = 0.18033688
        v_1 = amax * v_0
        v_2 = triton_helpers.maximum(m_i_copy_0, v_1)
        v_3 = 0.18033688
        v_4 = qk * v_3
        subscript = v_2[:, :, None]
        v_5 = v_4 - subscript
        v_6 = libdevice.exp2(v_5)
        l_ij = tl.cast(tl.sum(v_6, 2), tl.float32)
        v_7 = m_i_copy_0 - v_2
        v_8 = libdevice.exp2(v_7)
        v_9 = l_i_copy_0 * v_8
        l_i = v_9 + l_ij
        subscript_1 = v_8[:, :, None]
        v_11 = acc_copy_0 * subscript_1
        v = tl.load(v_view + (indices_0[:, None, None] * 32768 + indices_2[None, :, None] * 64 + indices_4[None, None, :] * 1), None)
        acc = tl.reshape(tl.dot(tl.reshape(tl.cast(v_6, tl.float32), [_BLOCK_SIZE_1, _BLOCK_SIZE_3]), tl.reshape(tl.cast(v, tl.float32), [_BLOCK_SIZE_3, 64]), acc=tl.reshape(v_11, [_BLOCK_SIZE_1, 64]), input_precision='tf32', out_dtype=tl.float32), [1, _BLOCK_SIZE_1, 64])
        m_i = v_2
    subscript_2 = l_i[:, :, None]
    v_12 = acc / subscript_2
    tl.store(out + (indices_0[:, None, None] * 32768 + indices_1[None, :, None] * 64 + indices_4[None, None, :] * 1), v_12, None)

def attention(q_in: torch.Tensor, k_in: torch.Tensor, v_in: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention.

    Implements the attention mechanism: Attention(Q, K, V) = softmax(Q * K^T / sqrt(d_k)) * V

    Args:
        q_in: Query tensor of shape [..., seq_len_q, head_dim]
        k_in: Key tensor of shape [..., seq_len_k, head_dim]
        v_in: Value tensor of shape [..., seq_len_k, head_dim]

    Returns:
        Output tensor of shape [..., seq_len_q, head_dim]
    """
    m_dim = q_in.size(-2)
    n_dim = k_in.size(-2)
    assert n_dim == v_in.size(-2)
    head_dim = 64
    assert head_dim == k_in.size(-1) == v_in.size(-1)
    q_view = q_in.reshape([-1, m_dim, head_dim])
    v_view = v_in.reshape([-1, n_dim, head_dim])
    k_view = k_in.reshape([-1, n_dim, head_dim]).transpose(1, 2)
    out = torch.empty_like(q_view)
    _BLOCK_SIZE_1 = 64
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_3 = 64
    _launcher(_helion_attention, (32 * triton.cdiv(512, _BLOCK_SIZE_1),), q_view, k_view, v_view, out, _BLOCK_SIZE_1, _RDIM_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.view(q_in.size())

--- assertExpectedJournal(TestExamples.test_bmm)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_bmm(A, B, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = tl.cdiv(16, _BLOCK_SIZE_0)
    num_blocks_1 = tl.cdiv(512, _BLOCK_SIZE_1)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    offset_2 = pid_2 * _BLOCK_SIZE_2
    indices_2 = (offset_2 + tl.arange(0, _BLOCK_SIZE_2)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2], 0.0, tl.float32)
    for offset_3 in tl.range(0, 768, _BLOCK_SIZE_3):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(A + (indices_0[:, None, None] * 393216 + indices_1[None, :, None] * 768 + indices_3[None, None, :] * 1), None)
        load_1 = tl.load(B + (indices_0[:, None, None] * 786432 + indices_3[None, :, None] * 1024 + indices_2[None, None, :] * 1), None)
        acc = tl.dot(tl.cast(load, tl.float16), tl.cast(load_1, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    v_0 = tl.cast(acc, tl.float16)
    tl.store(out + (indices_0[:, None, None] * 524288 + indices_1[None, :, None] * 1024 + indices_2[None, None, :] * 1), v_0, None)

def bmm(A: torch.Tensor, B: torch.Tensor, *, _launcher=_default_launcher):
    """
    Performs batch matrix multiplication.

    Args:
        A: Input tensor of shape [B, M, K]
        B: Input tensor of shape [B, K, N]

    Returns:
        Output tensor of shape [B, M, N] containing the result of batch matrix multiplication
    """
    b, m, k = A.size()
    b, k, n = B.size()
    out = torch.empty([b, m, n], device=A.device, dtype=torch.promote_types(A.dtype, B.dtype))
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _BLOCK_SIZE_3 = 16
    _launcher(_helion_bmm, (triton.cdiv(16, _BLOCK_SIZE_0) * triton.cdiv(512, _BLOCK_SIZE_1) * triton.cdiv(1024, _BLOCK_SIZE_2),), A, B, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_concat)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_concat2d_dim1(x, out, y, out_size_1, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, y_stride_0, y_stride_1, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    num_blocks_0 = tl.cdiv(x_size_0, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < out_size_1
    v_0 = tl.cast(x_size_1, tl.int32)
    v_1 = indices_1 < v_0
    subscript = v_1[None, :]
    x_part = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :] & subscript, other=0)
    v_2 = tl.cast(x_size_1, tl.int32)
    v_3 = indices_1 - v_2
    v_4 = tl.cast(x_size_1, tl.int32)
    v_5 = indices_1 >= v_4
    subscript_1 = v_5[None, :]
    y_part = tl.load(y + (indices_0[:, None] * y_stride_0 + v_3[None, :] * y_stride_1), mask_0[:, None] & mask_1[None, :] & subscript_1, other=0)
    v_6 = tl.cast(x_size_1, tl.int32)
    v_7 = indices_1 < v_6
    subscript_2 = v_7[None, :]
    v_8 = tl.where(subscript_2, x_part, y_part)
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_8, mask_0[:, None] & mask_1[None, :])

def concat2d_dim1(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """
    Concatenates two 2D tensors along dimension 1 (columns).

    Args:
        x: First input tensor of shape [M, N1]
        y: Second input tensor of shape [M, N2] with same first dimension as x

    Returns:
        Output tensor of shape [M, N1+N2] containing the concatenation of x and y along dimension 1
    """
    assert x.size(0) == y.size(0)
    out = torch.empty([x.size(0), x.size(1) + y.size(1)], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 32
    _BLOCK_SIZE_1 = 32
    _launcher(_helion_concat2d_dim1, (triton.cdiv(x.size(0), _BLOCK_SIZE_0) * triton.cdiv(out.size(1), _BLOCK_SIZE_1),), x, out, y, out.size(1), x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), y.stride(0), y.stride(1), _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_concat_block_ptr)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_concat2d_dim1(x, out, y, out_size_0, out_size_1, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, y_stride_0, y_stride_1, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    num_blocks_0 = tl.cdiv(x_size_0, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_size_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < out_size_1
    v_0 = tl.cast(x_size_1, tl.int32)
    v_1 = indices_1 < v_0
    subscript = v_1[None, :]
    x_part = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :] & subscript, other=0)
    v_2 = tl.cast(x_size_1, tl.int32)
    v_3 = indices_1 - v_2
    v_4 = tl.cast(x_size_1, tl.int32)
    v_5 = indices_1 >= v_4
    subscript_1 = v_5[None, :]
    y_part = tl.load(y + (indices_0[:, None] * y_stride_0 + v_3[None, :] * y_stride_1), mask_0[:, None] & mask_1[None, :] & subscript_1, other=0)
    v_6 = tl.cast(x_size_1, tl.int32)
    v_7 = indices_1 < v_6
    subscript_2 = v_7[None, :]
    v_8 = tl.where(subscript_2, x_part, y_part)
    tl.store(tl.make_block_ptr(out, [out_size_0, out_size_1], [out_stride_0, out_stride_1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_8, boundary_check=[0, 1])

def concat2d_dim1(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """
    Concatenates two 2D tensors along dimension 1 (columns).

    Args:
        x: First input tensor of shape [M, N1]
        y: Second input tensor of shape [M, N2] with same first dimension as x

    Returns:
        Output tensor of shape [M, N1+N2] containing the concatenation of x and y along dimension 1
    """
    assert x.size(0) == y.size(0)
    out = torch.empty([x.size(0), x.size(1) + y.size(1)], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 128
    _BLOCK_SIZE_1 = 64
    _launcher(_helion_concat2d_dim1, (triton.cdiv(x.size(0), _BLOCK_SIZE_0) * triton.cdiv(out.size(1), _BLOCK_SIZE_1),), x, out, y, out.size(0), out.size(1), x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), y.stride(0), y.stride(1), _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_cross_entropy)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_helpers import math as tl_math
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_cross_entropy(labels, logits_flat, logits, losses, labels_stride_0, logits_stride_0, logits_stride_1, logits_flat_stride_0, losses_stride_0, v, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < v
    labels_tile = tl.load(labels + indices_0 * labels_stride_0, None)
    v_0 = tl.cast(v, tl.int32)
    v_1 = indices_0 * v_0
    v_2 = tl.cast(v_1, tl.int64)
    v_3 = v_2 + labels_tile
    logits_at_target = tl.load(logits_flat + v_3 * logits_flat_stride_0, None)
    logits_rows = tl.load(logits + (indices_0[:, None] * logits_stride_0 + indices_1[None, :] * logits_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), logits_rows, tl.full([], float('-inf'), tl.float32))
    max_logits = tl.cast(tl.reshape(tl.max(_mask_to, 1), [1, 1]), tl.float32)
    v_4 = logits_rows - max_logits
    v_5 = libdevice.exp(v_4)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_5, tl.full([], 0, tl.float32))
    sum_exp = tl.cast(tl.reshape(tl.sum(_mask_to_1, 1), [1, 1]), tl.float32)
    squeeze = tl.reshape(max_logits, [1])
    squeeze_1 = tl.reshape(sum_exp, [1])
    v_6 = tl_math.log(squeeze_1)
    v_7 = squeeze + v_6
    v_8 = v_7 - logits_at_target
    tl.store(losses + indices_0 * losses_stride_0, v_8, None)

def cross_entropy(logits: torch.Tensor, labels: torch.Tensor, *, _launcher=_default_launcher):
    """
    Computes the cross entropy loss between logits and target labels.

    Implements the cross entropy loss function commonly used in classification tasks.
    The function computes the log softmax of the logits and then calculates the negative
    log likelihood of the true labels.

    Args:
        logits: Input logits tensor of shape [N, V] where N is batch size and V is vocabulary size
        labels: Target labels tensor of shape [N] containing class indices

    Returns:
        A scalar tensor containing the mean cross entropy loss
    """
    n, v = logits.shape
    losses = torch.zeros([n], dtype=logits.dtype, device=logits.device)
    logits_flat = logits.view(-1)
    _RDIM_SIZE_1 = triton.next_power_of_2(v)
    _launcher(_helion_cross_entropy, (n,), labels, logits_flat, logits, losses, labels.stride(0), logits.stride(0), logits.stride(1), logits_flat.stride(0), losses.stride(0), v, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return losses.mean()

--- assertExpectedJournal(TestExamples.test_embedding_block_ptr)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_embedding(x_flat, weight, out, out_size_0, out_size_1, x_flat_size_0, out_stride_0, out_stride_1, weight_stride_0, weight_stride_1, x_flat_stride_0, embedding_dim, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    pid_1 = tl.program_id(1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < x_flat_size_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < embedding_dim
    load = tl.load(tl.make_block_ptr(x_flat, [x_flat_size_0], [x_flat_stride_0], [offset_0], [_BLOCK_SIZE_0], [0]), boundary_check=[0], padding_option='zero')
    load_1 = tl.load(weight + (load[:, None] * weight_stride_0 + indices_1[None, :] * weight_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    tl.store(tl.make_block_ptr(out, [out_size_0, out_size_1], [out_stride_0, out_stride_1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), load_1, boundary_check=[0, 1])

def embedding(x: torch.Tensor, weight: torch.Tensor, *, _launcher=_default_launcher):
    """
    Performs embedding lookup for input indices.

    Maps indices in the input tensor to vectors from the embedding weight matrix.

    Args:
        x: Input tensor of indices of any shape
        weight: Embedding weight matrix of shape [num_embeddings, embedding_dim]

    Returns:
        Output tensor of shape [*x.shape, embedding_dim] containing the embedding vectors
    """
    x_flat = x.reshape(-1)
    _, embedding_dim = weight.size()
    out = torch.empty([x_flat.size(0), embedding_dim], dtype=weight.dtype, device=weight.device)
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 64
    _launcher(_helion_embedding, (triton.cdiv(x_flat.size(0), _BLOCK_SIZE_0), triton.cdiv(embedding_dim, _BLOCK_SIZE_1)), x_flat, weight, out, out.size(0), out.size(1), x_flat.size(0), out.stride(0), out.stride(1), weight.stride(0), weight.stride(1), x_flat.stride(0), embedding_dim, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return out.view(*x.size(), embedding_dim)

--- assertExpectedJournal(TestExamples.test_embedding_pointers)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_embedding(x_flat, weight, out, x_flat_size_0, out_stride_0, out_stride_1, weight_stride_0, weight_stride_1, x_flat_stride_0, embedding_dim, _BLOCK_SIZE_1: tl.constexpr):
    num_blocks_0 = x_flat_size_0
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < embedding_dim
    load = tl.load(x_flat + indices_0 * x_flat_stride_0, None)
    load_1 = tl.load(weight + (load[:, None] * weight_stride_0 + indices_1[None, :] * weight_stride_1), mask_1[None, :], other=0)
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), load_1, mask_1[None, :])

def embedding(x: torch.Tensor, weight: torch.Tensor, *, _launcher=_default_launcher):
    """
    Performs embedding lookup for input indices.

    Maps indices in the input tensor to vectors from the embedding weight matrix.

    Args:
        x: Input tensor of indices of any shape
        weight: Embedding weight matrix of shape [num_embeddings, embedding_dim]

    Returns:
        Output tensor of shape [*x.shape, embedding_dim] containing the embedding vectors
    """
    x_flat = x.reshape(-1)
    _, embedding_dim = weight.size()
    out = torch.empty([x_flat.size(0), embedding_dim], dtype=weight.dtype, device=weight.device)
    _BLOCK_SIZE_1 = 256
    _launcher(_helion_embedding, (x_flat.size(0) * triton.cdiv(embedding_dim, _BLOCK_SIZE_1),), x_flat, weight, out, x_flat.size(0), out.stride(0), out.stride(1), weight.stride(0), weight.stride(1), x_flat.stride(0), embedding_dim, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return out.view(*x.size(), embedding_dim)

--- assertExpectedJournal(TestExamples.test_fp8_attention)
from __future__ import annotations

import math
import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_fp8_attention_kernel(q, k, v, out, out_stride_0, heads, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_5 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    for offset_4 in tl.range(0, 256, _BLOCK_SIZE_1):
        indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        m_i = tl.full([_BLOCK_SIZE_1], float('-inf'), tl.float32)
        l_i = tl.full([_BLOCK_SIZE_1], 0.0, tl.float32)
        acc = tl.full([_BLOCK_SIZE_1, 64], 0.0, tl.float32)
        q_tile = tl.load(q + (offset_0 * 16384 + indices_4[:, None] * 64 + indices_5[None, :] * 1), None)
        for offset_2 in tl.range(0, 256, _BLOCK_SIZE_3):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
            q_tile_copy = q_tile
            m_i_copy = m_i
            l_i_copy = l_i
            acc_copy = acc
            q_tile_copy_0 = q_tile_copy
            m_i_copy_0 = m_i_copy
            l_i_copy_0 = l_i_copy
            acc_copy_0 = acc_copy
            k_tile = tl.load(k + (offset_0 * 16384 + indices_2[:, None] * 64 + indices_5[None, :] * 1), None)
            k_tile_t = tl.permute(k_tile, [1, 0])
            qk = tl.dot(tl.cast(q_tile_copy_0, tl.float8e4nv), tl.cast(k_tile_t, tl.float8e4nv), input_precision='tf32', out_dtype=tl.float32)
            v_0 = 0.18033688
            v_1 = qk * v_0
            qk_max = tl.cast(tl.max(v_1, 1), tl.float32)
            v_2 = triton_helpers.maximum(m_i_copy_0, qk_max)
            subscript = v_2[:, None]
            v_3 = v_1 - subscript
            v_4 = libdevice.exp2(v_3)
            l_ij = tl.cast(tl.sum(v_4, 1), tl.float32)
            v_5 = m_i_copy_0 - v_2
            v_6 = libdevice.exp2(v_5)
            v_7 = l_i_copy_0 * v_6
            l_i = v_7 + l_ij
            subscript_1 = v_6[:, None]
            v_9 = acc_copy_0 * subscript_1
            v_tile = tl.load(v + (offset_0 * 16384 + indices_5[:, None] * 1 + indices_2[None, :] * 64), None)
            v_10 = tl.cast(v_4, tl.float8e4nv)
            v_t = tl.permute(v_tile, [1, 0])
            acc = tl.dot(tl.cast(v_10, tl.float8e4nv), tl.cast(v_t, tl.float8e4nv), acc=v_9, input_precision='tf32', out_dtype=tl.float32)
            m_i = v_2
        subscript_2 = l_i[:, None]
        v_11 = acc / subscript_2
        v_12 = tl.cast(v_11, tl.float8e4nv)
        symnode_0 = triton_helpers.div_floor_integer(offset_0, heads)
        symnode_1 = triton_helpers.remainder_integer(offset_0, heads)
        tl.store(out + (symnode_0 * out_stride_0 + symnode_1 * 16384 + indices_4[:, None] * 64 + indices_5[None, :] * 1), v_12, None)

def fp8_attention_kernel(q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, batch: int, heads: int, *, _launcher=_default_launcher):
    """
    Computes scaled dot-product attention using FP8 precision.
    Implements the attention with FP8 tensors for improved performance and memory efficiency.
    Args:
        q: Query tensor of shape [batch*heads, seq, dim] in FP8 format
        k: Key tensor of shape [batch*heads, seq, dim] in FP8 format
        v: Value tensor of shape [batch*heads, dim, seq] (pre-transposed) in FP8 format
        batch: Number of batches
        heads: Number of attention heads
    Returns:
        Output tensor of shape [batch, heads, seq_len, head_dim] in FP8 format
    """
    batch_heads = q.size(0)
    seq_len = q.size(1)
    head_dim = q.size(2)
    out = torch.empty([batch, heads, seq_len, head_dim], dtype=torch.float8_e4m3fn, device=q.device)
    sm_scale = 1.0 / math.sqrt(float(head_dim))
    sm_scale = sm_scale * 1.44269504
    _RDIM_SIZE_2 = 64
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_3 = 64
    _launcher(_helion_fp8_attention_kernel, (8,), q, k, v, out, out.stride(0), heads, _RDIM_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_fp8_gemm)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_fp8_gemm(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_blocks_0 = tl.cdiv(256, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 256, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        x_tile = tl.load(x + (indices_0[:, None] * 256 + indices_2[None, :] * 1), None)
        y_tile = tl.load(y + (indices_2[:, None] * 256 + indices_1[None, :] * 1), None)
        acc = tl.dot(tl.cast(x_tile, tl.float8e4nv), tl.cast(y_tile, tl.float8e4nv), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    v_0 = tl.cast(acc, tl.float16)
    tl.store(out + (indices_0[:, None] * 256 + indices_1[None, :] * 1), v_0, None)

def fp8_gemm(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """
    FP8 General Matrix Multiplication (GEMM).
    This kernel demonstrates FP8 computation in Helion.
    When lowered to Triton, the tl.dot operation will handle
    FP8 inputs natively and accumulate to FP32.
    Args:
        x (torch.Tensor): Input tensor of shape [m, k] in FP8 format.
        y (torch.Tensor): Input tensor of shape [k, n] in FP8 format.
    Returns:
        torch.Tensor: Output tensor of shape [m, n] in FP16 format.
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.float16, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 32
    _launcher(_helion_fp8_gemm, (triton.cdiv(256, _BLOCK_SIZE_0) * triton.cdiv(256, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_geglu)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_geglu(a_flat, b_flat, out_flat, a_flat_stride_0, b_flat_stride_0, out_flat_stride_0, total_elements, _BLOCK_SIZE_0: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < total_elements
    load = tl.load(a_flat + indices_0 * a_flat_stride_0, mask_0, other=0)
    v_0 = tl.cast(load, tl.float32)
    b_vals = tl.load(b_flat + indices_0 * b_flat_stride_0, mask_0, other=0)
    v_1 = v_0 * v_0
    v_2 = v_1 * v_0
    v_3 = 0.044715
    v_4 = v_2 * v_3
    v_5 = v_0 + v_4
    v_6 = 0.7978845608028654
    v_7 = v_5 * v_6
    v_8 = libdevice.tanh(v_7)
    v_9 = 0.5
    v_10 = v_0 * v_9
    v_11 = 1.0
    v_12 = v_8 + v_11
    v_13 = v_10 * v_12
    v_14 = tl.cast(v_13, tl.float16)
    v_15 = v_14 * b_vals
    tl.store(out_flat + indices_0 * out_flat_stride_0, v_15, mask_0)

def geglu(a: Tensor, b: Tensor, *, _launcher=_default_launcher):
    """
    Performs GEGLU operation: GELU(a) * b using tanh approximation for GELU.

    GELU(a) = 0.5 * a * (1 + tanh(sqrt(2/π) * (a + 0.044715 * a³)))
    GEGLU(a, b) = GELU(a) * b

    Args:
        a (Tensor): Input tensor for GELU activation of any shape.
        b (Tensor): Input tensor for multiplication, must have same shape as a.

    Returns:
        Tensor: Result of GEGLU operation with same shape as inputs.
    """
    assert a.shape == b.shape, f'Input tensors must have same shape, got {a.shape} != {b.shape}'
    out = torch.empty_like(a, dtype=torch.promote_types(a.dtype, b.dtype))
    total_elements = a.numel()
    a_flat = a.view(-1)
    b_flat = b.view(-1)
    out_flat = out.view(-1)
    _BLOCK_SIZE_0 = 16
    _launcher(_helion_geglu, (triton.cdiv(total_elements, _BLOCK_SIZE_0),), a_flat, b_flat, out_flat, a_flat.stride(0), b_flat.stride(0), out_flat.stride(0), total_elements, _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_jagged_dense_add)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_jagged_dense_add_2d(x_offsets, x_data, y, out, y_size_1, out_stride_0, out_stride_1, x_data_stride_0, x_offsets_stride_0, y_stride_0, y_stride_1, num_rows, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, tl.full([], -9223372036854775808, tl.int64))
    max_nnz = tl.cast(tl.max(_mask_to, 0), tl.int64)
    for offset_1 in tl.range(0, max_nnz.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_nnz
        starts_copy = starts
        v_2_copy = v_2
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = starts_copy_0[:, None]
        subscript_1 = indices_1[None, :]
        v_3 = tl.cast(subscript_1, tl.int64)
        v_4 = subscript + v_3
        subscript_2 = indices_1[None, :]
        subscript_3 = v_2_copy_0[:, None]
        v_5 = tl.cast(subscript_2, tl.int64)
        v_6 = v_5 < subscript_3
        x_slice = tl.load(x_data + v_4 * x_data_stride_0, mask_0[:, None] & mask_1[None, :] & v_6, other=0)
        load_1 = tl.load(y + (indices_0[:, None] * y_stride_0 + indices_1[None, :] * y_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
        v_7 = load_1 + x_slice
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_7, mask_0[:, None] & mask_1[None, :])
    for offset_2 in tl.range(max_nnz.to(tl.int32), y_size_1.to(tl.int32), _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        mask_2 = indices_2 < y_size_1
        load = tl.load(y + (indices_0[:, None] * y_stride_0 + indices_2[None, :] * y_stride_1), mask_0[:, None] & mask_2[None, :], other=0)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_2[None, :] * out_stride_1), load, mask_0[:, None] & mask_2[None, :])

def jagged_dense_add_2d(x_data: torch.Tensor, x_offsets: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    """
    Add a jagged-prefix sparse tensor (x_data, x_offsets) to a dense matrix y
    and return the dense result.

    Args:
        x_data: 1-D tensor holding all non-zero elements row-by-row
        x_offsets: (num_rows + 1) tensor. Row i is the slice
                   x_data[x_offsets[i] : x_offsets[i+1]] (length K_i)
        y: (num_rows, N) tensor, N >= max(K_i)

    Returns:
        Dense tensor of shape (num_rows, N) containing the sum of the jagged and dense tensors
    """
    num_rows = y.size(0)
    assert x_offsets.size(0) == num_rows + 1
    out = torch.zeros_like(y)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_jagged_dense_add_2d, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_data, y, out, y.size(1), out.stride(0), out.stride(1), x_data.stride(0), x_offsets.stride(0), y.stride(0), y.stride(1), num_rows, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_jagged_hstu_attn)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion__helion_jagged_attention_kernel(seq_offsets, q, k, v, out, k_stride_0, k_stride_1, k_stride_2, out_stride_0, out_stride_1, out_stride_2, q_stride_0, q_stride_1, q_stride_2, seq_offsets_stride_0, v_stride_0, v_stride_1, v_stride_2, max_seq_len, alpha, scale, _BLOCK_SIZE_2: tl.constexpr, _RDIM_SIZE_3: tl.constexpr, _BLOCK_SIZE_4: tl.constexpr):
    num_blocks_0 = 4
    num_blocks_1 = 8
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0
    offset_1 = pid_1
    offset_2 = pid_2 * _BLOCK_SIZE_2
    indices_2 = (offset_2 + tl.arange(0, _BLOCK_SIZE_2)).to(tl.int32)
    mask_2 = indices_2 < max_seq_len
    indices_5 = tl.arange(0, _RDIM_SIZE_3).to(tl.int32)
    starts = tl.load(seq_offsets + offset_0 * seq_offsets_stride_0, None)
    add = 1 + offset_0
    ends = tl.load(seq_offsets + add * seq_offsets_stride_0, None)
    v_0 = ends - starts
    v_1 = v_0 > offset_2
    if v_1:
        v_0_copy = v_0
        starts_copy = starts
        v_0_copy_0 = v_0_copy
        starts_copy_0 = starts_copy
        v_2 = v_0_copy_0[None]
        v_3 = tl.cast(v_2, tl.int32)
        v_4 = indices_2 < v_3
        v_5 = starts_copy_0[None]
        v_6 = tl.cast(v_5, tl.int32)
        v_7 = indices_2 + v_6
        q_blk = tl.load(q + (v_7[:, None] * q_stride_0 + offset_1 * q_stride_1 + indices_5[None, :] * q_stride_2), mask_2[:, None], other=0)
        acc = tl.full([_BLOCK_SIZE_2, 32], 0.0, tl.float32)
        tile_end = tl.minimum(offset_2 + _BLOCK_SIZE_2, max_seq_len)
        for offset_3 in tl.range(0, tile_end.to(tl.int32), _BLOCK_SIZE_4):
            indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_4).to(tl.int32)
            mask_4 = indices_3 < tile_end
            v_0_copy_0_copy = v_0_copy_0
            starts_copy_0_copy = starts_copy_0
            q_blk_copy = q_blk
            v_4_copy = v_4
            acc_copy = acc
            v_0_copy_0_copy_0 = v_0_copy_0_copy
            starts_copy_0_copy_0 = starts_copy_0_copy
            q_blk_copy_0 = q_blk_copy
            v_4_copy_0 = v_4_copy
            acc_copy_0 = acc_copy
            v_8 = v_0_copy_0_copy_0[None]
            v_9 = tl.cast(v_8, tl.int32)
            v_10 = indices_3 < v_9
            v_11 = starts_copy_0_copy_0[None]
            v_12 = tl.cast(v_11, tl.int32)
            v_13 = indices_3 + v_12
            k_blk = tl.load(k + (v_13[:, None] * k_stride_0 + offset_1 * k_stride_1 + indices_5[None, :] * k_stride_2), mask_4[:, None], other=0)
            v_14 = starts_copy_0_copy_0[None]
            v_15 = tl.cast(v_14, tl.int32)
            v_16 = indices_3 + v_15
            v_blk = tl.load(v + (v_16[:, None] * v_stride_0 + offset_1 * v_stride_1 + indices_5[None, :] * v_stride_2), mask_4[:, None], other=0)
            permute = tl.permute(k_blk, [1, 0])
            mm = tl.dot(tl.cast(q_blk_copy_0, tl.bfloat16), tl.cast(permute, tl.bfloat16), input_precision='tf32', out_dtype=tl.float32)
            v_17 = tl.cast(alpha, tl.bfloat16)
            v_18 = mm * v_17
            v_19 = tl.cast(v_18, tl.float32)
            v_20 = tl.sigmoid(tl.cast(v_19, tl.float32))
            v_21 = v_19 * v_20
            v_22 = tl.cast(v_21, tl.bfloat16)
            v_23 = tl.cast(scale, tl.bfloat16)
            v_24 = v_22 * v_23
            unsqueeze = indices_2[:, None]
            unsqueeze_1 = indices_3[None, :]
            v_25 = unsqueeze > unsqueeze_1
            subscript = v_4_copy_0[:, None]
            v_26 = v_25 & subscript
            subscript_1 = v_10[None, :]
            v_27 = v_26 & subscript_1
            v_28 = 0.0
            v_29 = v_28[None, None]
            v_30 = tl.where(v_27, v_24, v_29)
            _mask_to_2 = tl.where(mask_2[:, None] & mask_4[None, :], v_30, tl.full([], 0, tl.bfloat16))
            mm_1 = tl.dot(tl.cast(_mask_to_2, tl.bfloat16), tl.cast(v_blk, tl.bfloat16), input_precision='tf32', out_dtype=tl.float32)
            v_31 = tl.cast(mm_1, tl.float32)
            acc = acc_copy_0 + v_31
        v_33 = tl.cast(acc, tl.bfloat16)
        v_34 = starts_copy_0[None]
        v_35 = tl.cast(v_34, tl.int32)
        v_36 = indices_2 + v_35
        tl.store(out + (v_36[:, None] * out_stride_0 + offset_1 * out_stride_1 + indices_5[None, :] * out_stride_2), v_33, mask_2[:, None])

def _helion_jagged_attention_kernel(max_seq_len: int, alpha: float, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor, seq_offsets: torch.Tensor, *, _launcher=_default_launcher):
    """Helion implementation of HSTU jagged attention"""
    scale = 1.0 / max_seq_len
    out = torch.zeros_like(v)
    _BLOCK_SIZE_2 = 16
    _RDIM_SIZE_3 = 32
    _BLOCK_SIZE_4 = 16
    _launcher(_helion__helion_jagged_attention_kernel, (4 * q.size(1) * triton.cdiv(max_seq_len, _BLOCK_SIZE_2),), seq_offsets, q, k, v, out, k.stride(0), k.stride(1), k.stride(2), out.stride(0), out.stride(1), out.stride(2), q.stride(0), q.stride(1), q.stride(2), seq_offsets.stride(0), v.stride(0), v.stride(1), v.stride(2), max_seq_len, alpha, scale, _BLOCK_SIZE_2, _RDIM_SIZE_3, _BLOCK_SIZE_4, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_jagged_mean)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_jagged_mean_kernel(x_offsets, x_feature_counts, x_flat, out, out_stride_0, out_stride_1, x_feature_counts_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, max_M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, tl.full([], -9223372036854775808, tl.int64))
    max_nnz = tl.cast(tl.max(_mask_to, 0), tl.int64)
    feature_counts = tl.load(x_feature_counts + indices_0 * x_feature_counts_stride_0, mask_0, other=0)
    for offset_1 in tl.range(0, max_M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < max_M
        feature_counts_copy = feature_counts
        max_nnz_copy = max_nnz
        starts_copy = starts
        v_2_copy = v_2
        feature_counts_copy_0 = feature_counts_copy
        max_nnz_copy_0 = max_nnz_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        subscript = feature_counts_copy_0[:, None]
        v_3 = indices_1[None, :]
        v_4 = v_3 < subscript
        row_sums = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_nnz_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_nnz_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            v_4_copy = v_4
            row_sums_copy = row_sums
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            v_4_copy_0 = v_4_copy
            row_sums_copy_0 = row_sums_copy
            subscript_1 = starts_copy_0_copy_0[:, None]
            subscript_2 = indices_2[None, :]
            v_5 = tl.cast(subscript_2, tl.int64)
            v_6 = subscript_1 + v_5
            subscript_3 = v_6[:, :, None]
            v_7 = subscript_3 * max_M
            subscript_4 = indices_1[None, None, :]
            v_8 = tl.cast(subscript_4, tl.int64)
            v_9 = v_7 + v_8
            subscript_5 = indices_2[None, :]
            subscript_6 = v_2_copy_0_copy_0[:, None]
            v_10 = tl.cast(subscript_5, tl.int64)
            v_11 = v_10 < subscript_6
            subscript_7 = v_11[:, :, None]
            subscript_8 = v_4_copy_0[:, None, :]
            v_12 = subscript_7 & subscript_8
            x_slice = tl.load(x_flat + v_9 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            sum_1 = tl.cast(tl.sum(x_slice, 1), tl.float32)
            row_sums = row_sums_copy_0 + sum_1
        v_14 = tl.cast(v_2_copy_0, tl.float32)
        nnz_expanded = v_14[:, None]
        v_15 = 0.0
        v_16 = nnz_expanded > v_15
        v_17 = row_sums / nnz_expanded
        v_18 = 0.0
        v_19 = v_18[None, None]
        v_20 = tl.where(v_16, v_17, v_19)
        v_21 = 0.0
        v_22 = v_21[None, None]
        v_23 = tl.where(v_4, v_20, v_22)
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_23, mask_0[:, None] & mask_1[None, :])

def jagged_mean_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, x_feature_counts: torch.Tensor, max_M: int, *, _launcher=_default_launcher):
    """
    Compute the mean of each row in a jagged tensor with variable features per row.

    Args:
        x_data: 2-D tensor of shape (total_elements, max_M) holding all elements
        x_offsets: (num_rows + 1) tensor. Row i is the slice
                   x_data[x_offsets[i] : x_offsets[i+1], :]
        x_feature_counts: (num_rows) tensor. Number of valid features for each row
        max_M: Maximum number of features

    Returns:
        2-D tensor of shape (num_rows, max_M) containing the mean of each row.
        Invalid features (beyond x_feature_counts[i]) are set to 0.
    """
    num_rows = x_offsets.size(0) - 1
    out = torch.zeros([num_rows, max_M], dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 8
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_jagged_mean_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_feature_counts, x_flat, out, out.stride(0), out.stride(1), x_feature_counts.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, max_M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_jagged_softmax)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_jagged_softmax_kernel(x_offsets, x_flat, out, out_stride_0, x_flat_stride_0, x_offsets_stride_0, num_rows, M, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_rows
    starts = tl.load(x_offsets + indices_0 * x_offsets_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    ends = tl.load(x_offsets + v_1 * x_offsets_stride_0, mask_0, other=0)
    v_2 = ends - starts
    _mask_to = tl.where(mask_0, v_2, tl.full([], -9223372036854775808, tl.int64))
    max_seqlen = tl.cast(tl.max(_mask_to, 0), tl.int64)
    for offset_1 in tl.range(0, M.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < M
        max_seqlen_copy = max_seqlen
        starts_copy = starts
        v_2_copy = v_2
        max_seqlen_copy_0 = max_seqlen_copy
        starts_copy_0 = starts_copy
        v_2_copy_0 = v_2_copy
        block_max = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        block_new_max = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        block_L = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
        for offset_2 in tl.range(0, max_seqlen_copy_0.to(tl.int32), _BLOCK_SIZE_2):
            indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_2 < max_seqlen_copy_0
            starts_copy_0_copy = starts_copy_0
            v_2_copy_0_copy = v_2_copy_0
            block_max_copy = block_max
            block_L_copy = block_L
            starts_copy_0_copy_0 = starts_copy_0_copy
            v_2_copy_0_copy_0 = v_2_copy_0_copy
            block_max_copy_0 = block_max_copy
            block_L_copy_0 = block_L_copy
            subscript = starts_copy_0_copy_0[:, None]
            subscript_1 = indices_2[None, :]
            v_3 = tl.cast(subscript_1, tl.int64)
            v_4 = subscript + v_3
            subscript_2 = v_4[:, :, None]
            v_5 = subscript_2 * M
            subscript_3 = indices_1[None, None, :]
            v_6 = tl.cast(subscript_3, tl.int64)
            v_7 = v_5 + v_6
            subscript_4 = indices_2[None, :]
            subscript_5 = v_2_copy_0_copy_0[:, None]
            v_8 = tl.cast(subscript_4, tl.int64)
            v_9 = v_8 < subscript_5
            subscript_6 = v_9[:, :, None]
            v_10 = tl.cast(M, tl.int32)
            v_11 = indices_1 < v_10
            subscript_7 = v_11[None, None, :]
            v_12 = subscript_6 & subscript_7
            x_slice = tl.load(x_flat + v_7 * x_flat_stride_0, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :] & v_12, other=0)
            v_13 = float('-inf')
            v_14 = v_13[None, None, None]
            v_15 = tl.where(v_12, x_slice, v_14)
            _mask_to_1 = tl.where(mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :], v_15, tl.full([], float('-inf'), tl.float32))
            slice_max = tl.cast(tl.max(_mask_to_1, 1), tl.float32)
            block_new_max = triton_helpers.maximum(block_max_copy_0, slice_max)
            v_17 = block_max_copy_0 - block_new_max
            v_18 = libdevice.exp(v_17)
            v_19 = block_L_copy_0 * v_18
            subscript_8 = block_new_max[:, None, :]
            v_20 = x_slice - subscript_8
            v_21 = float('-inf')
            v_22 = v_21[None, None, None]
            v_23 = tl.where(v_12, v_20, v_22)
            v_24 = libdevice.exp(v_23)
            _mask_to_2 = tl.where(mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :], v_24, tl.full([], 0, tl.float32))
            sum_1 = tl.cast(tl.sum(_mask_to_2, 1), tl.float32)
            block_L = v_19 + sum_1
            block_max = block_new_max
        for offset_3 in tl.range(0, max_seqlen_copy_0.to(tl.int32), _BLOCK_SIZE_3):
            indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
            mask_3 = indices_3 < max_seqlen_copy_0
            starts_copy_0_copy_1 = starts_copy_0
            v_2_copy_0_copy_1 = v_2_copy_0
            block_max_copy_1 = block_max
            block_L_copy_1 = block_L
            starts_copy_0_copy_1_0 = starts_copy_0_copy_1
            v_2_copy_0_copy_1_0 = v_2_copy_0_copy_1
            block_max_copy_1_0 = block_max_copy_1
            block_L_copy_1_0 = block_L_copy_1
            subscript_9 = starts_copy_0_copy_1_0[:, None]
            subscript_10 = indices_3[None, :]
            v_26 = tl.cast(subscript_10, tl.int64)
            v_27 = subscript_9 + v_26
            subscript_11 = v_27[:, :, None]
            v_28 = subscript_11 * M
            subscript_12 = indices_1[None, None, :]
            v_29 = tl.cast(subscript_12, tl.int64)
            v_30 = v_28 + v_29
            subscript_13 = indices_3[None, :]
            subscript_14 = v_2_copy_0_copy_1_0[:, None]
            v_31 = tl.cast(subscript_13, tl.int64)
            v_32 = v_31 < subscript_14
            subscript_15 = v_32[:, :, None]
            v_33 = tl.cast(M, tl.int32)
            v_34 = indices_1 < v_33
            subscript_16 = v_34[None, None, :]
            v_35 = subscript_15 & subscript_16
            x_slice_1 = tl.load(x_flat + v_30 * x_flat_stride_0, mask_0[:, None, None] & mask_3[None, :, None] & mask_1[None, None, :] & v_35, other=0)
            subscript_17 = block_max_copy_1_0[:, None, :]
            v_36 = x_slice_1 - subscript_17
            v_37 = libdevice.exp(v_36)
            subscript_18 = block_L_copy_1_0[:, None, :]
            v_38 = v_37 / subscript_18
            tl.store(out + v_30 * out_stride_0, v_38, mask_0[:, None, None] & mask_3[None, :, None] & mask_1[None, None, :] & v_35)

def jagged_softmax_kernel(x_data: torch.Tensor, x_offsets: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute the per-batch softmax in a jagged tensor.

    Args:
        x_data: 2-D tensor of shape (total_elements, max_M) holding all elements
        x_offsets: (num_rows + 1) tensor. Row i is the slice
                   x_data[x_offsets[i] : x_offsets[i+1], :]

    Returns:
        2-D tensor of shape (total_elements, max_M), containing the per-batch softmax scores.
    """
    N = int(x_offsets[-1].item())
    num_rows, M = (x_offsets.size(0) - 1, x_data.size(1))
    out = torch.zeros(N * M, dtype=x_data.dtype, device=x_data.device)
    x_flat = x_data.view(-1)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 8
    _BLOCK_SIZE_2 = 16
    _BLOCK_SIZE_3 = 16
    _launcher(_helion_jagged_softmax_kernel, (triton.cdiv(num_rows, _BLOCK_SIZE_0),), x_offsets, x_flat, out, out.stride(0), x_flat.stride(0), x_offsets.stride(0), num_rows, M, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out.reshape(N, M)

--- assertExpectedJournal(TestExamples.test_jsd)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_jsd_forward(_input, target, loss, dX, _input_stride_0, _input_stride_1, dX_stride_0, dX_stride_1, loss_stride_0, loss_stride_1, target_stride_0, target_stride_1, BT, V, beta, n_non_ignore, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < BT
    for offset_1 in tl.range(0, V.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < V
        X = tl.load(_input + (indices_0[:, None] * _input_stride_0 + indices_1[None, :] * _input_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
        Y = tl.load(target + (indices_0[:, None] * target_stride_0 + indices_1[None, :] * target_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
        _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], X, tl.full([], float('-inf'), tl.float32))
        X_max = tl.cast(tl.max(_mask_to, 0), tl.float32)
        _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], Y, tl.full([], float('-inf'), tl.float32))
        Y_max = tl.cast(tl.max(_mask_to_1, 0), tl.float32)
        eq = beta == 0.0
        if eq:
            Y_copy = Y
            Y_max_copy = Y_max
            X_copy = X
            Y_copy_0 = Y_copy
            Y_max_copy_0 = Y_max_copy
            X_copy_0 = X_copy
            v_0 = Y_max_copy_0[None, :]
            v_1 = Y_copy_0 - v_0
            v_2 = libdevice.exp(v_1)
            v_3 = libdevice.exp(Y_max_copy_0)
            v_4 = v_3[None, :]
            v_5 = v_2 * v_4
            v_6 = Y_copy_0 - X_copy_0
            v_7 = v_5 * v_6
            tl.store(loss + (indices_0[:, None] * loss_stride_0 + indices_1[None, :] * loss_stride_1), v_7, mask_0[:, None] & mask_1[None, :])
            v_8 = -v_5
            tl.store(dX + (indices_0[:, None] * dX_stride_0 + indices_1[None, :] * dX_stride_1), v_8, mask_0[:, None] & mask_1[None, :])
        _not = not eq
        if _not:
            X_copy_1 = X
            X_max_copy = X_max
            Y_copy_1 = Y
            Y_max_copy_1 = Y_max
            X_copy_1_0 = X_copy_1
            X_max_copy_0 = X_max_copy
            Y_copy_1_0 = Y_copy_1
            Y_max_copy_1_0 = Y_max_copy_1
            eq_1 = beta == 1.0
            if eq_1:
                X_copy_1_0_copy = X_copy_1_0
                X_max_copy_0_copy = X_max_copy_0
                Y_copy_1_0_copy = Y_copy_1_0
                X_copy_1_0_copy_0 = X_copy_1_0_copy
                X_max_copy_0_copy_0 = X_max_copy_0_copy
                Y_copy_1_0_copy_0 = Y_copy_1_0_copy
                v_9 = X_max_copy_0_copy_0[None, :]
                v_10 = X_copy_1_0_copy_0 - v_9
                v_11 = libdevice.exp(v_10)
                v_12 = libdevice.exp(X_max_copy_0_copy_0)
                v_13 = v_12[None, :]
                v_14 = v_11 * v_13
                v_15 = X_copy_1_0_copy_0 - Y_copy_1_0_copy_0
                v_16 = v_14 * v_15
                tl.store(loss + (indices_0[:, None] * loss_stride_0 + indices_1[None, :] * loss_stride_1), v_16, mask_0[:, None] & mask_1[None, :])
                load = tl.load(loss + (indices_0[:, None] * loss_stride_0 + indices_1[None, :] * loss_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
                v_17 = load + v_14
                tl.store(dX + (indices_0[:, None] * dX_stride_0 + indices_1[None, :] * dX_stride_1), v_17, mask_0[:, None] & mask_1[None, :])
            _not_1 = not eq_1
            if _not_1:
                X_max_copy_0_copy_1 = X_max_copy_0
                Y_max_copy_1_0_copy = Y_max_copy_1_0
                X_copy_1_0_copy_1 = X_copy_1_0
                Y_copy_1_0_copy_1 = Y_copy_1_0
                X_max_copy_0_copy_1_0 = X_max_copy_0_copy_1
                Y_max_copy_1_0_copy_0 = Y_max_copy_1_0_copy
                X_copy_1_0_copy_1_0 = X_copy_1_0_copy_1
                Y_copy_1_0_copy_1_0 = Y_copy_1_0_copy_1
                v_18 = triton_helpers.maximum(X_max_copy_0_copy_1_0, Y_max_copy_1_0_copy_0)
                v_19 = v_18[None, :]
                v_20 = X_copy_1_0_copy_1_0 - v_19
                v_21 = v_18[None, :]
                v_22 = Y_copy_1_0_copy_1_0 - v_21
                v_23 = libdevice.exp(v_18)
                v_24 = libdevice.exp(v_20)
                v_25 = v_23[None, :]
                v_26 = v_24 * v_25
                v_27 = libdevice.exp(v_22)
                v_28 = v_23[None, :]
                v_29 = v_27 * v_28
                v_30 = v_29 * beta
                sub_2 = 1.0 + -1 * beta
                v_31 = v_26 * sub_2
                v_32 = v_30 + v_31
                v_33 = tl_math.log(v_32)
                v_34 = v_30 * Y_copy_1_0_copy_1_0
                v_35 = v_31 * X_copy_1_0_copy_1_0
                v_36 = v_34 + v_35
                v_37 = v_32 * v_33
                v_38 = v_36 - v_37
                tl.store(loss + (indices_0[:, None] * loss_stride_0 + indices_1[None, :] * loss_stride_1), v_38, mask_0[:, None] & mask_1[None, :])
                v_39 = X_copy_1_0_copy_1_0 - v_33
                v_40 = v_31 * v_39
                tl.store(dX + (indices_0[:, None] * dX_stride_0 + indices_1[None, :] * dX_stride_1), v_40, mask_0[:, None] & mask_1[None, :])
        truediv = 1.0 / n_non_ignore
        load_2 = tl.load(loss + (indices_0[:, None] * loss_stride_0 + indices_1[None, :] * loss_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
        v_41 = load_2 * truediv
        tl.store(loss + (indices_0[:, None] * loss_stride_0 + indices_1[None, :] * loss_stride_1), v_41, mask_0[:, None] & mask_1[None, :])
        load_3 = tl.load(dX + (indices_0[:, None] * dX_stride_0 + indices_1[None, :] * dX_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
        v_42 = load_3 * truediv
        tl.store(dX + (indices_0[:, None] * dX_stride_0 + indices_1[None, :] * dX_stride_1), v_42, mask_0[:, None] & mask_1[None, :])

def jsd_forward(_input: Tensor, target: Tensor, shift_labels: Tensor | None=None, beta: float=0.5, ignore_index: int=-100, *, _launcher=_default_launcher):
    """
    Compute Jensen-Shannon Divergence loss.

    Args:
        _input: Student predictions in log-space, shape (BT, V)
        target: Teacher targets in log-space, shape (BT, V)
        shift_labels: Optional labels for masking, shape (BT,)
        beta: Coefficient for generalized JSD in [0, 1]
        ignore_index: Index to ignore in labels

    Returns:
        loss: Scalar JSD loss
        dX: Gradient of loss wrt input
    """
    BT, V = _input.shape
    assert target.shape == _input.shape, f'Shape mismatch: {target.shape} != {_input.shape}'
    n_rows = BT
    loss = torch.zeros(_input.shape, dtype=torch.float32, device=_input.device)
    dX = torch.empty_like(_input)
    n_non_ignore = float(BT)
    if shift_labels is not None:
        n_non_ignore = float((shift_labels != ignore_index).sum().item())
        if n_non_ignore == 0:
            return (torch.zeros([], dtype=_input.dtype, device=_input.device), torch.zeros_like(_input))
    BT_SIZE = helion.cdiv(BT, n_rows)
    _BLOCK_SIZE_0 = BT_SIZE
    _BLOCK_SIZE_1 = 4096
    _launcher(_helion_jsd_forward, (triton.cdiv(BT, _BLOCK_SIZE_0),), _input, target, loss, dX, _input.stride(0), _input.stride(1), dX.stride(0), dX.stride(1), loss.stride(0), loss.stride(1), target.stride(0), target.stride(1), BT, V, beta, n_non_ignore, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    final_loss = torch.sum(loss)
    return (final_loss, dX)

--- assertExpectedJournal(TestExamples.test_kl_div)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_helpers import math as tl_math
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_kl_div_forward(y_pred, y_true, kl_loss, loss, kl_loss_stride_0, kl_loss_stride_1, loss_stride_0, y_pred_stride_0, y_pred_stride_1, y_true_stride_0, y_true_stride_1, BT, V, log_target, eps, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_0: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_1 = pid_0 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < BT
    loss_sum = tl.full([_BLOCK_SIZE_1, _BLOCK_SIZE_0], 0.0, tl.float32)
    for offset_0 in tl.range(0, V.to(tl.int32), _BLOCK_SIZE_0):
        indices_0 = offset_0 + tl.arange(0, _BLOCK_SIZE_0).to(tl.int32)
        mask_0 = indices_0 < V
        loss_sum_copy = loss_sum
        loss_sum_copy_0 = loss_sum_copy
        y_pred_val = tl.load(y_pred + (indices_1[:, None] * y_pred_stride_0 + indices_0[None, :] * y_pred_stride_1), mask_1[:, None] & mask_0[None, :], other=0)
        y_true_val = tl.load(y_true + (indices_1[:, None] * y_true_stride_0 + indices_0[None, :] * y_true_stride_1), mask_1[:, None] & mask_0[None, :], other=0)
        if log_target:
            y_true_val_copy = y_true_val
            y_pred_val_copy = y_pred_val
            y_true_val_copy_0 = y_true_val_copy
            y_pred_val_copy_0 = y_pred_val_copy
            v_0 = libdevice.exp(y_true_val_copy_0)
            v_1 = y_true_val_copy_0 - y_pred_val_copy_0
            v_2 = v_0 * v_1
            tl.store(kl_loss + (indices_1[:, None] * kl_loss_stride_0 + indices_0[None, :] * kl_loss_stride_1), v_2, mask_1[:, None] & mask_0[None, :])
        _not = not log_target
        if _not:
            y_true_val_copy_1 = y_true_val
            y_pred_val_copy_1 = y_pred_val
            y_true_val_copy_1_0 = y_true_val_copy_1
            y_pred_val_copy_1_0 = y_pred_val_copy_1
            v_3 = triton_helpers.maximum(y_true_val_copy_1_0, eps)
            v_4 = tl_math.log(v_3)
            v_5 = v_4 - y_pred_val_copy_1_0
            v_6 = y_true_val_copy_1_0 * v_5
            tl.store(kl_loss + (indices_1[:, None] * kl_loss_stride_0 + indices_0[None, :] * kl_loss_stride_1), v_6, mask_1[:, None] & mask_0[None, :])
        load_2 = tl.load(kl_loss + (indices_1[:, None] * kl_loss_stride_0 + indices_0[None, :] * kl_loss_stride_1), mask_1[:, None] & mask_0[None, :], other=0)
        loss_sum = loss_sum_copy_0 + load_2
    sum_1 = tl.cast(tl.sum(loss_sum, 1), tl.float32)
    tl.store(loss + indices_1 * loss_stride_0, sum_1, mask_1)

def kl_div_forward(y_pred: Tensor, y_true: Tensor, log_target: bool=False, reduction: str='batchmean', eps: float=1e-10, *, _launcher=_default_launcher):
    """
    Compute KL Divergence loss.

    Args:
        y_pred: Input predictions in log-space, shape (BT, V)
        y_true: Target values (probabilities or log-probabilities), shape (BT, V)
        log_target: If True, y_true is in log-space; if False, y_true is probabilities
        reduction: Reduction mode ('none', 'sum', 'mean', 'batchmean')
        eps: Small value to avoid numerical issues

    Returns:
        loss: KL divergence loss
    """
    BT, V = y_pred.shape
    assert y_true.shape == y_pred.shape, f'Shape mismatch: {y_true.shape} != {y_pred.shape}'
    if reduction == 'none':
        loss = torch.zeros_like(y_pred)
    else:
        loss = torch.zeros((BT,), dtype=torch.float32, device=y_pred.device)
    kl_loss = torch.zeros_like(y_pred)
    BT_SIZE = helion.cdiv(BT, BT)
    _BLOCK_SIZE_1 = BT_SIZE
    _BLOCK_SIZE_0 = 4096
    _launcher(_helion_kl_div_forward, (triton.cdiv(BT, _BLOCK_SIZE_1),), y_pred, y_true, kl_loss, loss, kl_loss.stride(0), kl_loss.stride(1), loss.stride(0), y_pred.stride(0), y_pred.stride(1), y_true.stride(0), y_true.stride(1), BT, V, log_target, eps, _BLOCK_SIZE_1, _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    if reduction == 'batchmean':
        final_loss = torch.sum(loss) / BT
    elif reduction == 'sum':
        final_loss = torch.sum(loss, dim=0)
    elif reduction == 'mean':
        final_loss = torch.sum(loss) / (BT * V)
    else:
        final_loss = loss
    return final_loss

--- assertExpectedJournal(TestExamples.test_layernorm_bwd_dwdb)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_bwd_dwdb(x, grad_out, mean, rstd, dw, db, db_stride_0, dw_stride_0, grad_out_stride_0, grad_out_stride_1, mean_stride_0, rstd_stride_0, x_stride_0, x_stride_1, m, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_0: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_1 = pid_0 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_0 = tl.arange(0, _RDIM_SIZE_0).to(tl.int32)
    mask_0 = indices_0 < m
    rows = tl.arange(0, _RDIM_SIZE_0)
    load = tl.load(x + (rows[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None], other=0)
    v_0 = tl.cast(load, tl.float32)
    load_1 = tl.load(grad_out + (rows[:, None] * grad_out_stride_0 + indices_1[None, :] * grad_out_stride_1), mask_0[:, None], other=0)
    v_1 = tl.cast(load_1, tl.float32)
    mean_vec = tl.load(mean + rows * mean_stride_0, mask_0, other=0)
    rstd_vec = tl.load(rstd + rows * rstd_stride_0, mask_0, other=0)
    subscript = mean_vec[:, None]
    v_2 = v_0 - subscript
    subscript_1 = rstd_vec[:, None]
    v_3 = v_2 * subscript_1
    v_4 = v_1 * v_3
    sum_1 = tl.cast(tl.sum(v_4, 0), tl.float32)
    v_5 = tl.cast(sum_1, tl.float16)
    tl.store(dw + indices_1 * dw_stride_0, v_5, None)
    sum_2 = tl.cast(tl.sum(v_1, 0), tl.float32)
    v_6 = tl.cast(sum_2, tl.float16)
    tl.store(db + indices_1 * db_stride_0, v_6, None)

def layer_norm_bwd_dwdb(grad_out: torch.Tensor, x: torch.Tensor, mean: torch.Tensor, rstd: torch.Tensor, weight: torch.Tensor, compute_bias_grad: hl.constexpr=True, *, _launcher=_default_launcher):
    """
    Compute gradients for weight (dW) and optionally bias (dB) parameters.

    This kernel performs reduction across the batch dimension (M) to accumulate
    gradients for each feature dimension's weight and bias parameters.

    Args:
        grad_out: Gradient w.r.t layer norm output [M, N]
        x: Original input tensor [M, N]
        mean: Per-sample mean computed in forward pass [M]
        rstd: Per-sample reciprocal standard deviation from forward pass [M]
        weight: Weight parameter (used only for dtype/device info) [N]
        compute_bias_grad: Whether to compute bias gradient (default: True)

    Returns:
        (grad_weight, grad_bias): Gradients for weight and bias (if computed), both shape [N]
            grad_bias is None if compute_bias_grad is False
    """
    m, n = x.shape
    n = 64
    dw = torch.empty([n], dtype=weight.dtype, device=weight.device)
    if True:
        db = torch.empty([n], dtype=weight.dtype, device=weight.device)
    else:
        db = None
    _BLOCK_SIZE_1 = 32
    _RDIM_SIZE_0 = triton.next_power_of_2(m)
    _launcher(_helion_layer_norm_bwd_dwdb, (triton.cdiv(x.size(1), _BLOCK_SIZE_1),), x, grad_out, mean, rstd, dw, db, db.stride(0), dw.stride(0), grad_out.stride(0), grad_out.stride(1), mean.stride(0), rstd.stride(0), x.stride(0), x.stride(1), m, _BLOCK_SIZE_1, _RDIM_SIZE_0, num_warps=4, num_stages=3)
    if True:
        return (dw, db)
    return (dw, None)

--- assertExpectedJournal(TestExamples.test_layernorm_bwd_dwdb_no_bias)
from __future__ import annotations

import torch
import helion.language as hl
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_bwd_dwdb(x, grad_out, mean, rstd, dw, dw_stride_0, grad_out_stride_0, grad_out_stride_1, mean_stride_0, rstd_stride_0, x_stride_0, x_stride_1, m, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_0: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_1 = pid_0 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_0 = tl.arange(0, _RDIM_SIZE_0).to(tl.int32)
    mask_0 = indices_0 < m
    rows = tl.arange(0, _RDIM_SIZE_0)
    load = tl.load(x + (rows[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None], other=0)
    v_0 = tl.cast(load, tl.float32)
    load_1 = tl.load(grad_out + (rows[:, None] * grad_out_stride_0 + indices_1[None, :] * grad_out_stride_1), mask_0[:, None], other=0)
    v_1 = tl.cast(load_1, tl.float32)
    mean_vec = tl.load(mean + rows * mean_stride_0, mask_0, other=0)
    rstd_vec = tl.load(rstd + rows * rstd_stride_0, mask_0, other=0)
    subscript = mean_vec[:, None]
    v_2 = v_0 - subscript
    subscript_1 = rstd_vec[:, None]
    v_3 = v_2 * subscript_1
    v_4 = v_1 * v_3
    sum_1 = tl.cast(tl.sum(v_4, 0), tl.float32)
    v_5 = tl.cast(sum_1, tl.float16)
    tl.store(dw + indices_1 * dw_stride_0, v_5, None)

def layer_norm_bwd_dwdb(grad_out: torch.Tensor, x: torch.Tensor, mean: torch.Tensor, rstd: torch.Tensor, weight: torch.Tensor, compute_bias_grad: hl.constexpr=True, *, _launcher=_default_launcher):
    """
    Compute gradients for weight (dW) and optionally bias (dB) parameters.

    This kernel performs reduction across the batch dimension (M) to accumulate
    gradients for each feature dimension's weight and bias parameters.

    Args:
        grad_out: Gradient w.r.t layer norm output [M, N]
        x: Original input tensor [M, N]
        mean: Per-sample mean computed in forward pass [M]
        rstd: Per-sample reciprocal standard deviation from forward pass [M]
        weight: Weight parameter (used only for dtype/device info) [N]
        compute_bias_grad: Whether to compute bias gradient (default: True)

    Returns:
        (grad_weight, grad_bias): Gradients for weight and bias (if computed), both shape [N]
            grad_bias is None if compute_bias_grad is False
    """
    m, n = x.shape
    n = 64
    dw = torch.empty([n], dtype=weight.dtype, device=weight.device)
    if False:
        db = torch.empty([n], dtype=weight.dtype, device=weight.device)
    else:
        db = None
    _BLOCK_SIZE_1 = 32
    _RDIM_SIZE_0 = triton.next_power_of_2(m)
    _launcher(_helion_layer_norm_bwd_dwdb, (triton.cdiv(x.size(1), _BLOCK_SIZE_1),), x, grad_out, mean, rstd, dw, dw.stride(0), grad_out.stride(0), grad_out.stride(1), mean.stride(0), rstd.stride(0), x.stride(0), x.stride(1), m, _BLOCK_SIZE_1, _RDIM_SIZE_0, num_warps=4, num_stages=3)
    if False:
        return (dw, db)
    return (dw, None)

--- assertExpectedJournal(TestExamples.test_layernorm_bwd_dx)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_bwd_dx(x, grad_out, weight, mean, rstd, grad_x, grad_out_stride_0, grad_out_stride_1, grad_x_stride_0, grad_x_stride_1, mean_stride_0, rstd_stride_0, weight_stride_0, x_stride_0, x_stride_1, m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < m
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None], other=0)
    v_0 = tl.cast(load, tl.float32)
    load_1 = tl.load(grad_out + (indices_0[:, None] * grad_out_stride_0 + indices_1[None, :] * grad_out_stride_1), mask_0[:, None], other=0)
    v_1 = tl.cast(load_1, tl.float32)
    load_2 = tl.load(weight + indices_1 * weight_stride_0, None)
    v_2 = tl.cast(load_2, tl.float32)
    mean_tile = tl.load(mean + indices_0 * mean_stride_0, mask_0, other=0)
    rstd_tile = tl.load(rstd + indices_0 * rstd_stride_0, mask_0, other=0)
    subscript = mean_tile[:, None]
    v_3 = v_0 - subscript
    subscript_1 = rstd_tile[:, None]
    v_4 = v_3 * subscript_1
    v_5 = v_2[None, :]
    v_6 = v_5 * v_1
    v_7 = v_4 * v_6
    sum_1 = tl.cast(tl.sum(v_7, 1), tl.float32)
    v_8 = 0.015625
    v_9 = sum_1 * v_8
    sum_2 = tl.cast(tl.sum(v_6, 1), tl.float32)
    v_10 = 0.015625
    v_11 = sum_2 * v_10
    subscript_2 = v_9[:, None]
    v_12 = v_4 * subscript_2
    subscript_3 = v_11[:, None]
    v_13 = v_12 + subscript_3
    v_14 = v_6 - v_13
    subscript_4 = rstd_tile[:, None]
    v_15 = v_14 * subscript_4
    v_16 = tl.cast(v_15, tl.float16)
    tl.store(grad_x + (indices_0[:, None] * grad_x_stride_0 + indices_1[None, :] * grad_x_stride_1), v_16, mask_0[:, None])

def layer_norm_bwd_dx(grad_out: torch.Tensor, x: torch.Tensor, weight: torch.Tensor, mean: torch.Tensor, rstd: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute gradient for input tensor (dX).

    This kernel computes per-sample gradients by performing reductions across
    the feature dimension (N) for each sample in the batch.

    Args:
        grad_out: Gradient w.r.t layer norm output [M, N]
        x: Original input tensor [M, N]
        weight: Weight parameter [N]
        mean: Per-sample mean computed in forward pass [M]
        rstd: Per-sample reciprocal standard deviation from forward pass [M]

    Returns:
        grad_x: Gradient w.r.t input tensor, shape [M, N]
    """
    m, n = x.shape
    grad_x = torch.empty_like(x)
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    _launcher(_helion_layer_norm_bwd_dx, (triton.cdiv(m, _BLOCK_SIZE_0),), x, grad_out, weight, mean, rstd, grad_x, grad_out.stride(0), grad_out.stride(1), grad_x.stride(0), grad_x.stride(1), mean.stride(0), rstd.stride(0), weight.stride(0), x.stride(0), x.stride(1), m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return grad_x

--- assertExpectedJournal(TestExamples.test_layernorm_no_bias)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_fwd(x, weight, out, mean, rstd, mean_stride_0, out_stride_0, out_stride_1, rstd_stride_0, weight_stride_0, x_stride_0, x_stride_1, m, n, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < m
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < n
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    v_0 = tl.cast(load, tl.float32)
    sum_1 = tl.cast(tl.sum(v_0, 1), tl.float32)
    v_1 = tl.cast(n, tl.float32)
    v_2 = sum_1 / v_1
    subscript = v_2[:, None]
    v_3 = v_0 - subscript
    v_4 = v_3 * v_3
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_4, tl.full([], 0, tl.float32))
    sum_2 = tl.cast(tl.sum(_mask_to_1, 1), tl.float32)
    v_5 = tl.cast(n, tl.float32)
    v_6 = sum_2 / v_5
    v_7 = v_6 + eps
    v_8 = libdevice.rsqrt(v_7)
    subscript_1 = v_8[:, None]
    v_9 = v_3 * subscript_1
    load_1 = tl.load(weight + indices_1 * weight_stride_0, mask_1, other=0)
    v_10 = tl.cast(load_1, tl.float32)
    v_11 = v_10[None, :]
    v_12 = v_9 * v_11
    v_13 = tl.cast(v_12, tl.float16)
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_13, mask_0[:, None] & mask_1[None, :])
    tl.store(mean + indices_0 * mean_stride_0, v_2, mask_0)
    tl.store(rstd + indices_0 * rstd_stride_0, v_8, mask_0)

def layer_norm_fwd(x: torch.Tensor, normalized_shape: list[int], weight: torch.Tensor, bias: torch.Tensor | None=None, eps: float=1e-05, *, _launcher=_default_launcher):
    """
    Performs 1D layer normalization on the input tensor using Helion.
    Args:
        x (torch.Tensor): Input tensor of shape [batch_size, dim], expected to be FP16.
        normalized_shape (list[int]): List containing the dimension to normalize over (should be length 1).
        weight (torch.Tensor): Learnable scale parameter of shape [dim].
        bias (torch.Tensor | None): Optional learnable bias parameter of shape [dim].
        eps (float, optional): Small value added to variance for numerical stability. Default is 1e-5.
    Returns:
        tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
            - The layer-normalized output tensor of shape [batch_size, dim], in FP16.
            - Mean tensor of shape [batch_size], in FP32.
            - Reciprocal standard deviation tensor of shape [batch_size], in FP32.
    """
    m, n = x.size()
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {n}'
    if bias is not None:
        assert bias.size(0) == n, f'bias size mismatch {bias.size(0)} != {n}'
    assert len(normalized_shape) == 1, 'Helion layer norm only supports 1D layer norm currently'
    assert normalized_shape[0] == n, f'normalized shape mismatch {normalized_shape[0]} != {n}'
    out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    mean = torch.empty([m], dtype=torch.float32, device=x.device)
    rstd = torch.empty([m], dtype=torch.float32, device=x.device)
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = triton.next_power_of_2(n)
    _launcher(_helion_layer_norm_fwd, (triton.cdiv(m, _BLOCK_SIZE_0),), x, weight, out, mean, rstd, mean.stride(0), out.stride(0), out.stride(1), rstd.stride(0), weight.stride(0), x.stride(0), x.stride(1), m, n, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return (out, mean, rstd)

--- assertExpectedJournal(TestExamples.test_layernorm_with_bias)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_fwd(bias, x, weight, out, mean, rstd, bias_size_0, bias_stride_0, mean_stride_0, out_stride_0, out_stride_1, rstd_stride_0, weight_stride_0, x_stride_0, x_stride_1, m, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < m
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < bias_size_0
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    v_0 = tl.cast(load, tl.float32)
    sum_1 = tl.cast(tl.sum(v_0, 1), tl.float32)
    v_1 = tl.cast(bias_size_0, tl.float32)
    v_2 = sum_1 / v_1
    subscript = v_2[:, None]
    v_3 = v_0 - subscript
    v_4 = v_3 * v_3
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_4, tl.full([], 0, tl.float32))
    sum_2 = tl.cast(tl.sum(_mask_to_1, 1), tl.float32)
    v_5 = tl.cast(bias_size_0, tl.float32)
    v_6 = sum_2 / v_5
    v_7 = v_6 + eps
    v_8 = libdevice.rsqrt(v_7)
    subscript_1 = v_8[:, None]
    v_9 = v_3 * subscript_1
    load_1 = tl.load(weight + indices_1 * weight_stride_0, mask_1, other=0)
    v_10 = tl.cast(load_1, tl.float32)
    v_11 = v_10[None, :]
    v_12 = v_9 * v_11
    load_2 = tl.load(bias + indices_1 * bias_stride_0, mask_1, other=0)
    v_13 = tl.cast(load_2, tl.float32)
    v_14 = v_13[None, :]
    v_15 = v_12 + v_14
    v_16 = tl.cast(v_15, tl.float16)
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_16, mask_0[:, None] & mask_1[None, :])
    tl.store(mean + indices_0 * mean_stride_0, v_2, mask_0)
    tl.store(rstd + indices_0 * rstd_stride_0, v_8, mask_0)

def layer_norm_fwd(x: torch.Tensor, normalized_shape: list[int], weight: torch.Tensor, bias: torch.Tensor | None=None, eps: float=1e-05, *, _launcher=_default_launcher):
    """
    Performs 1D layer normalization on the input tensor using Helion.
    Args:
        x (torch.Tensor): Input tensor of shape [batch_size, dim], expected to be FP16.
        normalized_shape (list[int]): List containing the dimension to normalize over (should be length 1).
        weight (torch.Tensor): Learnable scale parameter of shape [dim].
        bias (torch.Tensor | None): Optional learnable bias parameter of shape [dim].
        eps (float, optional): Small value added to variance for numerical stability. Default is 1e-5.
    Returns:
        tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
            - The layer-normalized output tensor of shape [batch_size, dim], in FP16.
            - Mean tensor of shape [batch_size], in FP32.
            - Reciprocal standard deviation tensor of shape [batch_size], in FP32.
    """
    m, n = x.size()
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {n}'
    if bias is not None:
        assert bias.size(0) == n, f'bias size mismatch {bias.size(0)} != {n}'
    assert len(normalized_shape) == 1, 'Helion layer norm only supports 1D layer norm currently'
    assert normalized_shape[0] == n, f'normalized shape mismatch {normalized_shape[0]} != {n}'
    out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    mean = torch.empty([m], dtype=torch.float32, device=x.device)
    rstd = torch.empty([m], dtype=torch.float32, device=x.device)
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = triton.next_power_of_2(bias.size(0))
    _launcher(_helion_layer_norm_fwd, (triton.cdiv(m, _BLOCK_SIZE_0),), bias, x, weight, out, mean, rstd, bias.size(0), bias.stride(0), mean.stride(0), out.stride(0), out.stride(1), rstd.stride(0), weight.stride(0), x.stride(0), x.stride(1), m, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return (out, mean, rstd)

--- assertExpectedJournal(TestExamples.test_layernorm_without_bias)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_layer_norm_fwd(x, weight, out, mean, rstd, mean_stride_0, out_stride_0, out_stride_1, rstd_stride_0, weight_stride_0, x_stride_0, x_stride_1, m, n, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < m
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < n
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    v_0 = tl.cast(load, tl.float32)
    sum_1 = tl.cast(tl.sum(v_0, 1), tl.float32)
    v_1 = tl.cast(n, tl.float32)
    v_2 = sum_1 / v_1
    subscript = v_2[:, None]
    v_3 = v_0 - subscript
    v_4 = v_3 * v_3
    _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_4, tl.full([], 0, tl.float32))
    sum_2 = tl.cast(tl.sum(_mask_to_1, 1), tl.float32)
    v_5 = tl.cast(n, tl.float32)
    v_6 = sum_2 / v_5
    v_7 = v_6 + eps
    v_8 = libdevice.rsqrt(v_7)
    subscript_1 = v_8[:, None]
    v_9 = v_3 * subscript_1
    load_1 = tl.load(weight + indices_1 * weight_stride_0, mask_1, other=0)
    v_10 = tl.cast(load_1, tl.float32)
    v_11 = v_10[None, :]
    v_12 = v_9 * v_11
    v_13 = tl.cast(v_12, tl.float16)
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_13, mask_0[:, None] & mask_1[None, :])
    tl.store(mean + indices_0 * mean_stride_0, v_2, mask_0)
    tl.store(rstd + indices_0 * rstd_stride_0, v_8, mask_0)

def layer_norm_fwd(x: torch.Tensor, normalized_shape: list[int], weight: torch.Tensor, bias: torch.Tensor | None=None, eps: float=1e-05, *, _launcher=_default_launcher):
    """
    Performs 1D layer normalization on the input tensor using Helion.
    Args:
        x (torch.Tensor): Input tensor of shape [batch_size, dim], expected to be FP16.
        normalized_shape (list[int]): List containing the dimension to normalize over (should be length 1).
        weight (torch.Tensor): Learnable scale parameter of shape [dim].
        bias (torch.Tensor | None): Optional learnable bias parameter of shape [dim].
        eps (float, optional): Small value added to variance for numerical stability. Default is 1e-5.
    Returns:
        tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
            - The layer-normalized output tensor of shape [batch_size, dim], in FP16.
            - Mean tensor of shape [batch_size], in FP32.
            - Reciprocal standard deviation tensor of shape [batch_size], in FP32.
    """
    m, n = x.size()
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {n}'
    if bias is not None:
        assert bias.size(0) == n, f'bias size mismatch {bias.size(0)} != {n}'
    assert len(normalized_shape) == 1, 'Helion layer norm only supports 1D layer norm currently'
    assert normalized_shape[0] == n, f'normalized shape mismatch {normalized_shape[0]} != {n}'
    out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    mean = torch.empty([m], dtype=torch.float32, device=x.device)
    rstd = torch.empty([m], dtype=torch.float32, device=x.device)
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = triton.next_power_of_2(n)
    _launcher(_helion_layer_norm_fwd, (triton.cdiv(m, _BLOCK_SIZE_0),), x, weight, out, mean, rstd, mean.stride(0), out.stride(0), out.stride(1), rstd.stride(0), weight.stride(0), x.stride(0), x.stride(1), m, n, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return (out, mean, rstd)

--- assertExpectedJournal(TestExamples.test_matmul)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_matmul(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_pid_m = tl.cdiv(128, _BLOCK_SIZE_0)
    num_pid_n = tl.cdiv(128, _BLOCK_SIZE_1)
    inner_2d_pid = tl.program_id(0)
    num_pid_in_group = 4 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 4
    group_size_m = min(num_pid_m - first_pid_m, 4)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 128, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_0[:, None] * 128 + indices_2[None, :] * 1), None)
        load_1 = tl.load(y + (indices_2[:, None] * 128 + indices_1[None, :] * 1), None)
        acc = tl.dot(tl.cast(load, tl.float32), tl.cast(load_1, tl.float32), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    tl.store(out + (indices_0[:, None] * 128 + indices_1[None, :] * 1), acc, None)

def matmul(x: Tensor, y: Tensor, epilogue: Callable[[Tensor, tuple[Tensor, ...]], Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication of x and y with an optional epilogue function.
    Args:
        x (Tensor): Left matrix of shape [m, k].
        y (Tensor): Right matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        Tensor: Resulting matrix of shape [m, n].
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_matmul, (triton.cdiv(128, _BLOCK_SIZE_0) * triton.cdiv(128, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_matmul_layernorm_dynamic_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_matmul_layernorm(bias, x, y, weight, out, bias_size_0, bias_stride_0, out_stride_0, out_stride_1, weight_stride_0, x_stride_0, x_stride_1, y_stride_0, y_stride_1, m, k, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_0: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_1 = pid_0 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < m
    indices_0 = tl.arange(0, _RDIM_SIZE_0).to(tl.int32)
    mask_0 = indices_0 < bias_size_0
    acc = tl.full([_BLOCK_SIZE_1, _RDIM_SIZE_0], 0.0, tl.float32)
    for offset_2 in tl.range(0, k.to(tl.int32), _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        mask_2 = indices_2 < k
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_1[:, None] * x_stride_0 + indices_2[None, :] * x_stride_1), mask_1[:, None] & mask_2[None, :], other=0)
        load_1 = tl.load(y + (indices_2[:, None] * y_stride_0 + indices_0[None, :] * y_stride_1), mask_2[:, None] & mask_0[None, :], other=0)
        mm = tl.dot(tl.cast(load, tl.float32), tl.cast(load_1, tl.float32), input_precision='tf32', out_dtype=tl.float32)
        acc = acc_copy_0 + mm
    _mask_to = tl.where(mask_1[:, None] & mask_0[None, :], acc, tl.full([], 0, tl.float32))
    var_mean_extra = tl.cast(tl.reshape(tl.sum(_mask_to, 1), [_BLOCK_SIZE_1, 1]), tl.float32)
    v_1 = var_mean_extra / bias_size_0.to(tl.float32)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[:, None], [_BLOCK_SIZE_1, 1]), v_1, tl.full([], 0, tl.float32))
    v_2 = _mask_to - _mask_to_1
    v_3 = v_2 * v_2
    var_mean_extra_2 = tl.cast(tl.reshape(tl.sum(v_3, 1), [_BLOCK_SIZE_1, 1]), tl.float32)
    v_4 = var_mean_extra_2 / bias_size_0.to(tl.float32)
    v_5 = acc - v_1
    v_6 = 1e-05
    v_7 = v_4 + v_6
    v_8 = libdevice.rsqrt(v_7)
    v_9 = v_5 * v_8
    load_2 = tl.load(weight + indices_0 * weight_stride_0, mask_0, other=0)
    v_10 = load_2[None, :]
    v_11 = v_9 * v_10
    load_3 = tl.load(bias + indices_0 * bias_stride_0, mask_0, other=0)
    v_12 = load_3[None, :]
    v_13 = v_11 + v_12
    tl.store(out + (indices_1[:, None] * out_stride_0 + indices_0[None, :] * out_stride_1), v_13, mask_1[:, None] & mask_0[None, :])

def matmul_layernorm(x: torch.Tensor, y: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication followed by layer normalization.

    Args:
        x: First input tensor of shape [M, K]
        y: Second input tensor of shape [K, N]
        weight: Layer normalization weight parameter of shape [N]
        bias: Layer normalization bias parameter of shape [N]

    Returns:
        Output tensor of shape [M, N] containing the result of matrix multiplication followed by layer normalization
    """
    m, k = x.size()
    k2 = y.size(0)
    n = y.size(1)
    assert k == k2, f'size mismatch {k} != {k2}'
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {n}'
    assert bias.size(0) == n, f'bias size mismatch {bias.size(0)} != {n}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _BLOCK_SIZE_1 = 16
    _RDIM_SIZE_0 = triton.next_power_of_2(bias.size(0))
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_matmul_layernorm, (triton.cdiv(m, _BLOCK_SIZE_1),), bias, x, y, weight, out, bias.size(0), bias.stride(0), out.stride(0), out.stride(1), weight.stride(0), x.stride(0), x.stride(1), y.stride(0), y.stride(1), m, k, _BLOCK_SIZE_1, _RDIM_SIZE_0, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_matmul_layernorm_static_shapes)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_matmul_layernorm(x, y, weight, bias, out, out_stride_0, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_0: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_1 = pid_0 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    indices_0 = tl.arange(0, _RDIM_SIZE_0).to(tl.int32)
    mask_0 = indices_0 < 400
    acc = tl.full([_BLOCK_SIZE_1, _RDIM_SIZE_0], 0.0, tl.float32)
    for offset_2 in tl.range(0, 256, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_1[:, None] * 256 + indices_2[None, :] * 1), None)
        load_1 = tl.load(y + (indices_2[:, None] * 400 + indices_0[None, :] * 1), mask_0[None, :], other=0)
        mm = tl.dot(tl.cast(load, tl.float32), tl.cast(load_1, tl.float32), input_precision='tf32', out_dtype=tl.float32)
        acc = acc_copy_0 + mm
    _mask_to = tl.where(tl.broadcast_to(mask_0[None, :], [_BLOCK_SIZE_1, _RDIM_SIZE_0]), acc, tl.full([], 0, tl.float32))
    var_mean_extra = tl.cast(tl.reshape(tl.sum(_mask_to, 1), [_BLOCK_SIZE_1, 1]), tl.float32)
    v_1 = 400
    v_2 = var_mean_extra / v_1.to(tl.float32)
    v_3 = _mask_to - v_2
    v_4 = v_3 * v_3
    var_mean_extra_2 = tl.cast(tl.reshape(tl.sum(v_4, 1), [_BLOCK_SIZE_1, 1]), tl.float32)
    v_5 = 400
    v_6 = var_mean_extra_2 / v_5.to(tl.float32)
    v_7 = acc - v_2
    v_8 = 1e-05
    v_9 = v_6 + v_8
    v_10 = libdevice.rsqrt(v_9)
    v_11 = v_7 * v_10
    load_2 = tl.load(weight + indices_0 * 1, mask_0, other=0)
    v_12 = load_2[None, :]
    v_13 = v_11 * v_12
    load_3 = tl.load(bias + indices_0 * 1, mask_0, other=0)
    v_14 = load_3[None, :]
    v_15 = v_13 + v_14
    tl.store(out + (indices_1[:, None] * out_stride_0 + indices_0[None, :] * 1), v_15, mask_0[None, :])

def matmul_layernorm(x: torch.Tensor, y: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication followed by layer normalization.

    Args:
        x: First input tensor of shape [M, K]
        y: Second input tensor of shape [K, N]
        weight: Layer normalization weight parameter of shape [N]
        bias: Layer normalization bias parameter of shape [N]

    Returns:
        Output tensor of shape [M, N] containing the result of matrix multiplication followed by layer normalization
    """
    m, k = x.size()
    k2 = y.size(0)
    n = y.size(1)
    assert k == k2, f'size mismatch {k} != {k2}'
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {n}'
    assert bias.size(0) == n, f'bias size mismatch {bias.size(0)} != {n}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _BLOCK_SIZE_1 = 16
    _RDIM_SIZE_0 = 512
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_matmul_layernorm, (triton.cdiv(128, _BLOCK_SIZE_1),), x, y, weight, bias, out, out.stride(0), _BLOCK_SIZE_1, _RDIM_SIZE_0, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_matmul_split_k)
from __future__ import annotations

import torch
import helion
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

import helion._testing.matmul_split_k as _source_module

@triton.jit
def _helion_matmul_split_k(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    num_blocks_0 = tl.cdiv(64, _BLOCK_SIZE_0)
    num_blocks_1 = tl.cdiv(64, _BLOCK_SIZE_1)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0 % num_blocks_1
    pid_2 = tl.program_id(0) // (num_blocks_0 * num_blocks_1)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    offset_2 = pid_2 * _BLOCK_SIZE_2
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    tile_end = tl.minimum(offset_2 + _BLOCK_SIZE_2, 1024)
    for offset_3 in tl.range(offset_2.to(tl.int32), tile_end.to(tl.int32), _BLOCK_SIZE_3):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
        mask_3 = indices_3 < tile_end
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_0[:, None] * 1024 + indices_3[None, :] * 1), mask_3[None, :], other=0)
        load_1 = tl.load(y + (indices_3[:, None] * 64 + indices_1[None, :] * 1), mask_3[:, None], other=0)
        acc = tl.dot(tl.cast(load, tl.float32), tl.cast(load_1, tl.float32), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    eq = offset_2 == 0
    if eq:
        acc_copy_1 = acc
        acc = acc_copy_1
    tl.atomic_add(out + (indices_0[:, None] * 64 + indices_1[None, :] * 1), acc, mask=None, sem='relaxed')

def matmul_split_k(x: torch.Tensor, y: torch.Tensor, epilogue: Callable[[torch.Tensor, tuple[torch.Tensor, ...]], torch.Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Matrix multiplication kernel using split-K parallelism.
    This kernel splits the reduction (K) dimension into multiple fragments to improve
    parallelism and performance, especially for large K. The results from each split
    are accumulated atomically into the output tensor. An optional epilogue function
    can be applied to the accumulator, e.g., for adding bias.
    Args:
        x (torch.Tensor): Left input matrix of shape [m, k].
        y (torch.Tensor): Right input matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        torch.Tensor: Resulting matrix of shape [m, n].
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.zeros([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    split_k = 8
    k_block = helion.next_power_of_2(helion.cdiv(k, split_k))
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = k_block
    _BLOCK_SIZE_3 = 32
    _launcher(_helion_matmul_split_k, (triton.cdiv(64, _BLOCK_SIZE_0) * triton.cdiv(64, _BLOCK_SIZE_1) * triton.cdiv(1024, _BLOCK_SIZE_2),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_moe_matmul_ogs)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_moe_matmul_ogs(expert_token_offsets, expert_token_counts, sorted_to_orig_token_idx, A, W, C, A_stride_0, A_stride_1, C_stride_0, C_stride_1, W_stride_0, W_stride_1, W_stride_2, expert_token_counts_stride_0, expert_token_offsets_stride_0, sorted_to_orig_token_idx_stride_0, max_T_per_expert, N, K, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_3: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    start = tl.load(expert_token_offsets + offset_0 * expert_token_offsets_stride_0, None)
    num_tokens = tl.load(expert_token_counts + offset_0 * expert_token_counts_stride_0, None)
    v_0 = tl.full([], 0, tl.int32)
    v_1 = num_tokens != v_0
    if v_1:
        num_tokens_copy = num_tokens
        start_copy = start
        num_tokens_copy_0 = num_tokens_copy
        start_copy_0 = start_copy
        for offset_1 in tl.range(0, max_T_per_expert.to(tl.int32), _BLOCK_SIZE_1):
            indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
            mask_1 = indices_1 < max_T_per_expert
            for offset_2 in tl.range(0, N.to(tl.int32), _BLOCK_SIZE_2):
                indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
                mask_2 = indices_2 < N
                num_tokens_copy_0_copy = num_tokens_copy_0
                start_copy_0_copy = start_copy_0
                num_tokens_copy_0_copy_0 = num_tokens_copy_0_copy
                start_copy_0_copy_0 = start_copy_0_copy
                v_2 = num_tokens_copy_0_copy_0[None]
                v_3 = indices_1 < v_2
                v_4 = tl.full([], 0, tl.int32)
                v_5 = v_4[None]
                v_6 = tl.where(v_3, indices_1, v_5)
                v_7 = start_copy_0_copy_0[None]
                v_8 = v_7 + v_6
                squeeze = tl.reshape(v_8, [_BLOCK_SIZE_1])
                expert_orig_token_indices = tl.load(sorted_to_orig_token_idx + squeeze * sorted_to_orig_token_idx_stride_0, mask_1, other=0)
                acc = tl.full([_BLOCK_SIZE_1, _BLOCK_SIZE_2], 0.0, tl.float32)
                for offset_3 in tl.range(0, K.to(tl.int32), _BLOCK_SIZE_3):
                    indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_3).to(tl.int32)
                    mask_3 = indices_3 < K
                    expert_orig_token_indices_copy = expert_orig_token_indices
                    acc_copy = acc
                    expert_orig_token_indices_copy_0 = expert_orig_token_indices_copy
                    acc_copy_0 = acc_copy
                    A_frag = tl.load(A + (expert_orig_token_indices_copy_0[:, None] * A_stride_0 + indices_3[None, :] * A_stride_1), mask_1[:, None] & mask_3[None, :], other=0)
                    W_frag = tl.load(W + (offset_0 * W_stride_0 + indices_3[:, None] * W_stride_1 + indices_2[None, :] * W_stride_2), mask_3[:, None] & mask_2[None, :], other=0)
                    acc = tl.dot(tl.cast(A_frag, tl.float16), tl.cast(W_frag, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
                existing_values = tl.load(C + (expert_orig_token_indices[:, None] * C_stride_0 + indices_2[None, :] * C_stride_1), mask_1[:, None] & mask_2[None, :], other=0)
                view = tl.reshape(v_3, [_BLOCK_SIZE_1, 1])
                mask_2d = tl.broadcast_to(view, [_BLOCK_SIZE_1, _BLOCK_SIZE_2])
                v_9 = tl.cast(acc, tl.float16)
                v_10 = tl.where(mask_2d, v_9, existing_values)
                tl.store(C + (expert_orig_token_indices[:, None] * C_stride_0 + indices_2[None, :] * C_stride_1), v_10, mask_1[:, None] & mask_2[None, :])

def moe_matmul_ogs(A: torch.Tensor, W: torch.Tensor, expert_token_counts: torch.Tensor, expert_token_offsets: torch.Tensor, sorted_to_orig_token_idx: torch.Tensor, max_T_per_expert: int, *, _launcher=_default_launcher):
    """
    Helion kernel implementing MoE matmul with Outer-Gather-Scatter.
    Args:
        A (torch.Tensor): Input activations of shape [T, K].
        W (torch.Tensor): Expert weights of shape [E, K, N].
        expert_token_counts (torch.Tensor): Number of tokens per expert [E].
        expert_token_offsets (torch.Tensor): Starting offsets of tokens per expert [E+1].
        sorted_to_orig_token_idx (torch.Tensor): Maps sorted token indices to original token indices [T].
        max_T_per_expert (int): Maximum number of tokens per expert.
    Returns:
        torch.Tensor: Output activations of shape [T, N].
    """
    T, K = A.shape
    E, _, N = W.shape
    C = torch.zeros(T, N, dtype=torch.promote_types(A.dtype, W.dtype), device=A.device)
    _BLOCK_SIZE_2 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_3 = 16
    _launcher(_helion_moe_matmul_ogs, (E,), expert_token_offsets, expert_token_counts, sorted_to_orig_token_idx, A, W, C, A.stride(0), A.stride(1), C.stride(0), C.stride(1), W.stride(0), W.stride(1), W.stride(2), expert_token_counts.stride(0), expert_token_offsets.stride(0), sorted_to_orig_token_idx.stride(0), max_T_per_expert, N, K, _BLOCK_SIZE_2, _BLOCK_SIZE_1, _BLOCK_SIZE_3, num_warps=4, num_stages=3)
    return C

--- assertExpectedJournal(TestExamples.test_rms_norm_bwd_dw)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_rms_norm_bwd_dw(x, grad_out, inv_rms, dw, dw_stride_0, grad_out_stride_0, grad_out_stride_1, inv_rms_stride_0, x_stride_0, x_stride_1, n, m, _BLOCK_SIZE_1: tl.constexpr, _RDIM_SIZE_0: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_1 = pid_0 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < n
    indices_0 = tl.arange(0, _RDIM_SIZE_0).to(tl.int32)
    mask_0 = indices_0 < m
    rows = tl.arange(0, _RDIM_SIZE_0)
    load = tl.load(x + (rows[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    v_0 = tl.cast(load, tl.float32)
    load_1 = tl.load(grad_out + (rows[:, None] * grad_out_stride_0 + indices_1[None, :] * grad_out_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    v_1 = tl.cast(load_1, tl.float32)
    load_2 = tl.load(inv_rms + rows[:, None] * inv_rms_stride_0, mask_0[:, None], other=0)
    v_2 = tl.cast(load_2, tl.float32)
    v_3 = v_0 * v_2
    v_4 = v_1 * v_3
    sum_1 = tl.cast(tl.sum(v_4, 0), tl.float32)
    v_5 = tl.cast(sum_1, tl.float16)
    tl.store(dw + indices_1 * dw_stride_0, v_5, mask_1)

def rms_norm_bwd_dw(grad_out: torch.Tensor, x: torch.Tensor, weight: torch.Tensor, inv_rms: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute gradients for weight (dW)

    This kernel performs reduction across the batch dimension (M) to accumulate
    gradients for each feature dimension's weight parameter.

    Args:
        grad_out: Gradient w.r.t rms norm output [M, N]
        x: Original input tensor [M, N]
        weight: Weight parameter (used only for dtype/device info) [N]
        inv_rms: Inverse RMS tensor [M, 1]

    Returns:
        grad_weight: Gradients for weight with shape [N]
    """
    m, n = x.shape
    dw = torch.empty([n], dtype=weight.dtype, device=weight.device)
    _BLOCK_SIZE_1 = 32
    _RDIM_SIZE_0 = triton.next_power_of_2(m)
    _launcher(_helion_rms_norm_bwd_dw, (triton.cdiv(n, _BLOCK_SIZE_1),), x, grad_out, inv_rms, dw, dw.stride(0), grad_out.stride(0), grad_out.stride(1), inv_rms.stride(0), x.stride(0), x.stride(1), n, m, _BLOCK_SIZE_1, _RDIM_SIZE_0, num_warps=4, num_stages=3)
    return dw

--- assertExpectedJournal(TestExamples.test_rms_norm_bwd_dx)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_rms_norm_bwd_dx(x, grad_out, weight, inv_rms, grad_x, grad_out_stride_0, grad_out_stride_1, grad_x_stride_0, grad_x_stride_1, inv_rms_stride_0, weight_stride_0, x_stride_0, x_stride_1, m, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < m
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None], other=0)
    v_0 = tl.cast(load, tl.float32)
    load_1 = tl.load(grad_out + (indices_0[:, None] * grad_out_stride_0 + indices_1[None, :] * grad_out_stride_1), mask_0[:, None], other=0)
    v_1 = tl.cast(load_1, tl.float32)
    load_2 = tl.load(weight + indices_1 * weight_stride_0, None)
    v_2 = tl.cast(load_2, tl.float32)
    load_3 = tl.load(inv_rms + indices_0[:, None] * inv_rms_stride_0, mask_0[:, None], other=0)
    v_3 = tl.cast(load_3, tl.float32)
    v_4 = v_2[None, :]
    v_5 = v_1 * v_4
    v_6 = v_0 * v_3
    v_7 = v_5 * v_6
    rowsum_dy_normed = tl.cast(tl.reshape(tl.sum(v_7, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_8 = 0.015625
    v_9 = v_3 * v_8
    v_10 = 64.0
    v_11 = v_5 * v_10
    v_12 = v_6 * rowsum_dy_normed
    v_13 = v_11 - v_12
    v_14 = v_9 * v_13
    v_15 = tl.cast(v_14, tl.float16)
    tl.store(grad_x + (indices_0[:, None] * grad_x_stride_0 + indices_1[None, :] * grad_x_stride_1), v_15, mask_0[:, None])

def rms_norm_bwd_dx(grad_out: torch.Tensor, x: torch.Tensor, weight: torch.Tensor, inv_rms: torch.Tensor, *, _launcher=_default_launcher):
    """
    Compute gradient for input tensor (dX).

    This kernel computes per-sample gradients by performing reductions across
    the feature dimension (N) for each sample in the batch.

    Args:
        grad_out: Gradient w.r.t rms norm output [M, N]
        x: Original input tensor [M, N]
        weight: Weight parameter [N]
        inv_rms: Inverse RMS tensor [M, 1]

    Returns:
        grad_x: Gradient w.r.t input tensor, shape [M, N]
    """
    m, n = x.shape
    grad_x = torch.empty_like(x)
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_1 = 64
    _launcher(_helion_rms_norm_bwd_dx, (triton.cdiv(m, _BLOCK_SIZE_0),), x, grad_out, weight, inv_rms, grad_x, grad_out.stride(0), grad_out.stride(1), grad_x.stride(0), grad_x.stride(1), inv_rms.stride(0), weight.stride(0), x.stride(0), x.stride(1), m, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return grad_x

--- assertExpectedJournal(TestExamples.test_rms_norm_fwd)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_rms_norm_fwd(x, weight, out, inv_rms, inv_rms_stride_0, out_stride_0, out_stride_1, weight_stride_0, x_stride_0, x_stride_1, m, n, eps, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < m
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < n
    load = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    v_0 = tl.cast(load, tl.float32)
    v_1 = v_0 * v_0
    mean_x_squared_extra = tl.cast(tl.reshape(tl.sum(v_1, 1), [_BLOCK_SIZE_0, 1]), tl.float32)
    v_2 = mean_x_squared_extra / n.to(tl.float32)
    v_3 = v_2 + eps
    v_4 = libdevice.rsqrt(v_3)
    v_5 = v_0 * v_4
    load_1 = tl.load(weight + indices_1 * weight_stride_0, mask_1, other=0)
    v_6 = tl.cast(load_1, tl.float32)
    v_7 = v_6[None, :]
    v_8 = v_5 * v_7
    v_9 = tl.cast(v_8, tl.float16)
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_9, mask_0[:, None] & mask_1[None, :])
    v_10 = tl.cast(v_4, tl.float16)
    tl.store(inv_rms + indices_0[:, None] * inv_rms_stride_0, v_10, mask_0[:, None])

def rms_norm_fwd(x: torch.Tensor, weight: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    """
    Performs Root Mean Square (RMS) normalization on the input tensor.

    RMS normalization normalizes by the root mean square of the elements:
    output = x / sqrt(mean(x^2) + eps) * weight

    Args:
        x: Input tensor of shape [M, N]
        weight: Scale parameter of shape [N]
        eps: Small constant for numerical stability

    Returns:
        Output tensor of shape [M, N] with RMS normalization applied
        RMS tensor of shape [M, 1] with RMS values for each element
    """
    m, n = x.size()
    assert weight.size(0) == n, f'weight size mismatch {weight.size(0)} != {n}'
    out = torch.empty_like(x)
    inv_rms = torch.empty([m, 1], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 16
    _RDIM_SIZE_1 = triton.next_power_of_2(n)
    _launcher(_helion_rms_norm_fwd, (triton.cdiv(m, _BLOCK_SIZE_0),), x, weight, out, inv_rms, inv_rms.stride(0), out.stride(0), out.stride(1), weight.stride(0), x.stride(0), x.stride(1), m, n, eps, _BLOCK_SIZE_0, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return (out, inv_rms)

--- assertExpectedJournal(TestExamples.test_segment_reduction)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

import helion._testing.segment_reduction as _source_module

@triton.jit
def combine_fn_helion_0(param_0, param_1, param_2, param_3):
    v_0 = param_1 == param_3
    v_1 = param_0 + param_2
    v_2 = tl.where(v_0, v_1, param_2)
    return (v_2, param_3)

@triton.jit
def _helion_segmented_reduction_helion(input_data, indices, output, indices_stride_0, input_data_stride_0, input_data_stride_1, output_stride_0, output_stride_1, num_elements, num_features, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    num_blocks_0 = tl.cdiv(num_elements, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < num_elements
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    mask_1 = indices_1 < num_features
    vals = tl.load(input_data + (indices_0[:, None] * input_data_stride_0 + indices_1[None, :] * input_data_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
    idxs = tl.load(indices + indices_0 * indices_stride_0, mask_0, other=0)
    v_0 = tl.full([], 1, tl.int32)
    v_1 = indices_0 + v_0
    sub = -1 + num_elements
    v_2 = tl.cast(sub, tl.int32)
    v_3 = indices_0 < v_2
    idxs_next = tl.load(indices + v_1 * indices_stride_0, mask_0 & v_3, other=0)
    v_4 = tl.cast(idxs, tl.float32)
    unsqueeze = v_4[:, None]
    expand = tl.broadcast_to(unsqueeze, [_BLOCK_SIZE_0, _BLOCK_SIZE_1])
    out_vals = tl.associative_scan((vals, expand), 0, combine_fn_helion_0)[0]
    v_5 = idxs != idxs_next
    _BLOCK_SIZE_0_ = _BLOCK_SIZE_0
    v_6 = tl.cast(_BLOCK_SIZE_0_, tl.int32)
    v_7 = indices_0 % v_6
    v_8 = tl.full([], 0, tl.int32)
    v_9 = v_7 != v_8
    v_10 = libdevice.signbit(v_7) != 0 if v_7.dtype is tl.float32 else v_7 < 0
    v_11 = libdevice.signbit(v_6) != 0 if v_6.dtype is tl.float32 else v_6 < 0
    v_12 = v_10 != v_11
    v_13 = v_9 & v_12
    v_14 = v_7 + v_6
    v_15 = tl.where(v_13, v_14, v_7)
    sub_1 = -1 + _BLOCK_SIZE_0
    v_16 = tl.cast(sub_1, tl.int32)
    v_17 = v_15 == v_16
    v_18 = v_5 | v_17
    unsqueeze_1 = v_18[:, None]
    v_19 = 0.0
    v_20 = v_19[None, None]
    v_21 = tl.where(unsqueeze_1, out_vals, v_20)
    tl.atomic_add(output + (idxs[:, None] * output_stride_0 + indices_1[None, :] * output_stride_1), v_21, mask=mask_0[:, None] & mask_1[None, :], sem='relaxed')

def segmented_reduction_helion(indices: torch.Tensor, input_data: torch.Tensor, num_nodes: int, *, _launcher=_default_launcher):
    """
    Performs segmented reduction using Helion.

    Reduces input data by summing values with the same index.

    Args:
        indices: Tensor of segment indices for each element
        input_data: Input tensor of shape [num_elements, num_features]
        num_nodes: Number of output nodes/segments

    Returns:
        Output tensor of shape [num_nodes, num_features] with reduced values
    """
    num_elements, num_features = input_data.shape
    output = torch.zeros((num_nodes, num_features), dtype=input_data.dtype, device=input_data.device)
    _BLOCK_SIZE_0 = 32
    _BLOCK_SIZE_1 = 32
    _launcher(_helion_segmented_reduction_helion, (triton.cdiv(num_elements, _BLOCK_SIZE_0) * triton.cdiv(num_features, _BLOCK_SIZE_1),), input_data, indices, output, indices.stride(0), input_data.stride(0), input_data.stride(1), output.stride(0), output.stride(1), num_elements, num_features, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return output

--- assertExpectedJournal(TestExamples.test_softmax)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax(x, out, out_size_0, out_size_1, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    load = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, 0], [1, _RDIM_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), load, tl.full([], float('-inf'), tl.float32))
    amax = tl.cast(tl.reshape(tl.max(_mask_to, 1), [1, 1]), tl.float32)
    v_0 = load - amax
    v_1 = libdevice.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, tl.full([], 0, tl.float32))
    sum_1 = tl.cast(tl.reshape(tl.sum(_mask_to_1, 1), [1, 1]), tl.float32)
    v_2 = v_1 / sum_1
    tl.store(tl.make_block_ptr(out, [out_size_0, out_size_1], [out_stride_0, out_stride_1], [offset_0, 0], [1, _RDIM_SIZE_1], [1, 0]), v_2, boundary_check=[0, 1])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_helion_softmax, (n,), x, out, out.size(0), out.size(1), x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    return out

--- assertExpectedJournal(TestExamples.test_softmax_decomposed)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax_decomposed(x, out, out_size_0, out_size_1, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    values = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, 0], [1, _RDIM_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), values, tl.full([], float('-inf'), tl.float32))
    amax = tl.cast(tl.reshape(tl.max(_mask_to, 1), [1, 1]), tl.float32)
    v_0 = values - amax
    v_1 = libdevice.exp(v_0)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_1, tl.full([], 0, tl.float32))
    sum_exp = tl.cast(tl.reshape(tl.sum(_mask_to_1, 1), [1, 1]), tl.float32)
    v_2 = v_1 / sum_exp
    tl.store(tl.make_block_ptr(out, [out_size_0, out_size_1], [out_stride_0, out_stride_1], [offset_0, 0], [1, _RDIM_SIZE_1], [1, 0]), v_2, boundary_check=[0, 1])

def softmax_decomposed(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Helion kernel implementing softmax by decomposing into max, exp, and normalization steps.
    This avoids using PyTorch's built-in softmax decomposition.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_helion_softmax_decomposed, (n,), x, out, out.size(0), out.size(1), x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=1)
    return out

--- assertExpectedJournal(TestExamples.test_softmax_looped)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax(x, out, out_size_0, out_size_1, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    amax_acc = tl.full([1, _REDUCTION_BLOCK_1], float('-inf'), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        load = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, roffset_1], [1, _REDUCTION_BLOCK_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), load, tl.full([], float('-inf'), tl.float32))
        v_0 = triton_helpers.maximum(amax_acc, _mask_to)
        amax_acc = v_0
    amax = tl.cast(tl.reshape(tl.max(amax_acc, 1), [1, 1]), tl.float32)
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy = amax
        load_1 = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, roffset_1], [1, _REDUCTION_BLOCK_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        v_1 = load_1 - amax_copy
        v_2 = libdevice.exp(v_1)
        _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _REDUCTION_BLOCK_1]), v_2, tl.full([], 0, tl.float32))
        v_3 = sum_1_acc + _mask_to_1
        sum_1_acc = v_3
    sum_1 = tl.cast(tl.reshape(tl.sum(sum_1_acc, 1), [1, 1]), tl.float32)
    for roffset_1 in tl.range(0, _m, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < _m
        amax_copy_1 = amax
        sum_1_copy = sum_1
        load_2 = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, roffset_1], [1, _REDUCTION_BLOCK_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        v_4 = load_2 - amax_copy_1
        v_5 = libdevice.exp(v_4)
        v_6 = v_5 / sum_1_copy
        tl.store(tl.make_block_ptr(out, [out_size_0, out_size_1], [out_stride_0, out_stride_1], [offset_0, roffset_1], [1, _REDUCTION_BLOCK_1], [1, 0]), v_6, boundary_check=[0, 1])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Simple Helion kernel wrapping PyTorch's softmax function.
    Args:
        x (torch.Tensor): Input tensor of shape [n, m].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    n, _m = x.size()
    out = torch.empty_like(x)
    _REDUCTION_BLOCK_1 = 32
    _launcher(_helion_softmax, (n,), x, out, out.size(0), out.size(1), x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _REDUCTION_BLOCK_1, num_warps=4, num_stages=1)
    return out

--- assertExpectedJournal(TestExamples.test_softmax_two_pass)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax_two_pass(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, m, n, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < m
    mi = tl.full([_BLOCK_SIZE_0], float('-inf'), tl.float32)
    di = tl.full([_BLOCK_SIZE_0], 0.0, tl.float32)
    for offset_2 in tl.range(0, n.to(tl.int32), _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_2 < n
        mi_copy = mi
        di_copy = di
        mi_copy_0 = mi_copy
        di_copy_0 = di_copy
        values = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_2[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
        _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], values, tl.full([], float('-inf'), tl.float32))
        local_amax = tl.cast(tl.max(_mask_to, 1), tl.float32)
        v_0 = triton_helpers.maximum(mi_copy_0, local_amax)
        v_1 = mi_copy_0 - v_0
        v_2 = libdevice.exp(v_1)
        v_3 = di_copy_0 * v_2
        subscript = v_0[:, None]
        v_4 = values - subscript
        v_5 = libdevice.exp(v_4)
        _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_5, tl.full([], 0, tl.float32))
        sum_1 = tl.cast(tl.sum(_mask_to_1, 1), tl.float32)
        di = v_3 + sum_1
        mi = v_0
    for offset_2 in tl.range(0, n.to(tl.int32), _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_2 = indices_2 < n
        mi_copy_1 = mi
        di_copy_1 = di
        mi_copy_1_0 = mi_copy_1
        di_copy_1_0 = di_copy_1
        values_1 = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_2[None, :] * x_stride_1), mask_0[:, None] & mask_2[None, :], other=0)
        subscript_1 = mi_copy_1_0[:, None]
        v_7 = values_1 - subscript_1
        v_8 = libdevice.exp(v_7)
        subscript_2 = di_copy_1_0[:, None]
        v_9 = v_8 / subscript_2
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_2[None, :] * out_stride_1), v_9, mask_0[:, None] & mask_2[None, :])

def softmax_two_pass(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Numerically optimized Helion kernel performing softmax in two passes.
    This version uses fewer passes but is less numerically stable.
    Args:
        x (torch.Tensor): Input tensor of shape [m, n].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    m, n = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 32
    _BLOCK_SIZE_1 = 32
    _launcher(_helion_softmax_two_pass, (triton.cdiv(m, _BLOCK_SIZE_0),), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), m, n, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_softmax_two_pass_block_ptr)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax_two_pass(x, out, out_size_0, out_size_1, x_size_0, x_size_1, out_stride_0, out_stride_1, x_stride_0, x_stride_1, m, n, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < m
    mi = tl.full([_BLOCK_SIZE_0], float('-inf'), tl.float32)
    di = tl.full([_BLOCK_SIZE_0], 0.0, tl.float32)
    for offset_2 in tl.range(0, n.to(tl.int32), _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_2 < n
        mi_copy = mi
        di_copy = di
        mi_copy_0 = mi_copy
        di_copy_0 = di_copy
        values = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        _mask_to = tl.where(mask_0[:, None] & mask_1[None, :], values, tl.full([], float('-inf'), tl.float32))
        local_amax = tl.cast(tl.max(_mask_to, 1), tl.float32)
        v_0 = triton_helpers.maximum(mi_copy_0, local_amax)
        v_1 = mi_copy_0 - v_0
        v_2 = libdevice.exp(v_1)
        v_3 = di_copy_0 * v_2
        subscript = v_0[:, None]
        v_4 = values - subscript
        v_5 = libdevice.exp(v_4)
        _mask_to_1 = tl.where(mask_0[:, None] & mask_1[None, :], v_5, tl.full([], 0, tl.float32))
        sum_1 = tl.cast(tl.sum(_mask_to_1, 1), tl.float32)
        di = v_3 + sum_1
        mi = v_0
    for offset_2 in tl.range(0, n.to(tl.int32), _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mi_copy_1 = mi
        di_copy_1 = di
        mi_copy_1_0 = mi_copy_1
        di_copy_1_0 = di_copy_1
        values_1 = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        subscript_1 = mi_copy_1_0[:, None]
        v_7 = values_1 - subscript_1
        v_8 = libdevice.exp(v_7)
        subscript_2 = di_copy_1_0[:, None]
        v_9 = v_8 / subscript_2
        tl.store(tl.make_block_ptr(out, [out_size_0, out_size_1], [out_stride_0, out_stride_1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_9, boundary_check=[0, 1])

def softmax_two_pass(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Numerically optimized Helion kernel performing softmax in two passes.
    This version uses fewer passes but is less numerically stable.
    Args:
        x (torch.Tensor): Input tensor of shape [m, n].
    Returns:
        torch.Tensor: Softmax output tensor of the same shape.
    """
    m, n = x.size()
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 8
    _BLOCK_SIZE_1 = 64
    _launcher(_helion_softmax_two_pass, (triton.cdiv(m, _BLOCK_SIZE_0),), x, out, out.size(0), out.size(1), x.size(0), x.size(1), out.stride(0), out.stride(1), x.stride(0), x.stride(1), m, n, _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_sum)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_sum_kernel(x, out, out_stride_0, x_stride_0, x_stride_1, n, _REDUCTION_BLOCK_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    sum_1_acc = tl.full([1, _REDUCTION_BLOCK_1], 0, tl.float32)
    for roffset_1 in tl.range(0, n, _REDUCTION_BLOCK_1):
        rindex_1 = roffset_1 + tl.arange(0, _REDUCTION_BLOCK_1).to(tl.int32)
        mask_1 = rindex_1 < n
        load = tl.load(x + (indices_0[:, None] * x_stride_0 + rindex_1[None, :] * x_stride_1), mask_1[None, :], other=0)
        v_0 = sum_1_acc + load
        sum_1_acc = v_0
    sum_1 = tl.cast(tl.sum(sum_1_acc, 1), tl.float32)
    tl.store(out + indices_0 * out_stride_0, sum_1, None)

def sum_kernel(x: torch.Tensor, *, _launcher=_default_launcher):
    """
    Sums a 2D tensor along the last dimension.

    Args:
        x: Input tensor of shape [M, N]

    Returns:
        Output tensor of shape [M] containing the sum of each row
    """
    m, n = x.shape
    out = torch.empty([m], dtype=x.dtype, device=x.device)
    _REDUCTION_BLOCK_1 = 32768
    _launcher(_helion_sum_kernel, (m,), x, out, out.stride(0), x.stride(0), x.stride(1), n, _REDUCTION_BLOCK_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_swiglu)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_swiglu(a_flat, b_flat, out_flat, a_flat_stride_0, b_flat_stride_0, out_flat_stride_0, total_elements, _BLOCK_SIZE_0: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < total_elements
    load = tl.load(a_flat + indices_0 * a_flat_stride_0, mask_0, other=0)
    v_0 = tl.cast(load, tl.float32)
    b_vals = tl.load(b_flat + indices_0 * b_flat_stride_0, mask_0, other=0)
    v_1 = tl.sigmoid(tl.cast(v_0, tl.float32))
    v_2 = v_0 * v_1
    v_3 = tl.cast(v_2, tl.float16)
    v_4 = v_3 * b_vals
    tl.store(out_flat + indices_0 * out_flat_stride_0, v_4, mask_0)

def swiglu(a: Tensor, b: Tensor, *, _launcher=_default_launcher):
    """
    Performs SwiGLU operation: SiLU(a) * b where SiLU is the Swish activation.

    SiLU(a) = a * sigmoid(a) = a / (1 + exp(-a))
    SwiGLU(a, b) = SiLU(a) * b

    Args:
        a (Tensor): Input tensor for SiLU activation of any shape.
        b (Tensor): Input tensor for multiplication, must have same shape as a.

    Returns:
        Tensor: Result of SwiGLU operation with same shape as inputs.
    """
    assert a.shape == b.shape, f'Input tensors must have same shape, got {a.shape} != {b.shape}'
    out = torch.empty_like(a, dtype=torch.promote_types(a.dtype, b.dtype))
    total_elements = a.numel()
    a_flat = a.view(-1)
    b_flat = b.view(-1)
    out_flat = out.view(-1)
    _BLOCK_SIZE_0 = 16
    _launcher(_helion_swiglu, (triton.cdiv(total_elements, _BLOCK_SIZE_0),), a_flat, b_flat, out_flat, a_flat.stride(0), b_flat.stride(0), out_flat.stride(0), total_elements, _BLOCK_SIZE_0, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestExamples.test_template_via_closure0)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

import test.test_examples as _global_source0

@triton.jit
def _helion_matmul(x, y, epilogue_closure_0, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_pid_m = tl.cdiv(1024, _BLOCK_SIZE_0)
    num_pid_n = tl.cdiv(1024, _BLOCK_SIZE_1)
    inner_2d_pid = tl.program_id(0)
    num_pid_in_group = 64 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 64
    group_size_m = min(num_pid_m - first_pid_m, 64)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    offset_1 = pid_1 * _BLOCK_SIZE_1
    indices_1 = (offset_1 + tl.arange(0, _BLOCK_SIZE_1)).to(tl.int32)
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(x + (indices_0[:, None] * 1024 + indices_2[None, :] * 1), None)
        load_1 = tl.load(y + (indices_2[:, None] * 1024 + indices_1[None, :] * 1), None)
        acc = tl.dot(tl.cast(load, tl.float16), tl.cast(load_1, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    load_2 = tl.load(epilogue_closure_0 + indices_1[None, :] * 1, None)
    v_0 = tl.cast(load_2, tl.float32)
    v_1 = acc + v_0
    v_2 = tl.full([], 0, tl.int32)
    v_3 = triton_helpers.maximum(v_2, v_1)
    v_4 = tl.cast(v_3, tl.float16)
    tl.store(out + (indices_0[:, None] * 1024 + indices_1[None, :] * 1), v_4, None)

def matmul(x: Tensor, y: Tensor, epilogue: Callable[[Tensor, tuple[Tensor, ...]], Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication of x and y with an optional epilogue function.
    Args:
        x (Tensor): Left matrix of shape [m, k].
        y (Tensor): Right matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        Tensor: Resulting matrix of shape [m, n].
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _BLOCK_SIZE_0 = 64
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_matmul, (triton.cdiv(1024, _BLOCK_SIZE_0) * triton.cdiv(1024, _BLOCK_SIZE_1),), x, y, epilogue.__closure__[0].cell_contents, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=2, num_stages=4)
    return out

--- assertExpectedJournal(TestExamples.test_template_via_closure1)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

import test.test_examples as _global_source0

@triton.jit
def _helion_matmul(x, y, epilogue_closure_0, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_pid_m = tl.cdiv(1024, _BLOCK_SIZE_0)
    num_pid_n = tl.cdiv(1024, _BLOCK_SIZE_1)
    inner_2d_pid = tl.program_id(0)
    num_pid_in_group = 64 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 64
    group_size_m = min(num_pid_m - first_pid_m, 64)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_2):
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(tl.make_block_ptr(x, [1024, 1024], [1024, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_2], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        load_1 = tl.load(tl.make_block_ptr(y, [1024, 1024], [1024, 1], [offset_2, offset_1], [_BLOCK_SIZE_2, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        acc = tl.dot(tl.cast(load, tl.float16), tl.cast(load_1, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    load_2 = tl.load(tl.make_block_ptr(epilogue_closure_0, [1, 1024], [1024, 1], [0, offset_1], [1, _BLOCK_SIZE_1], [1, 0]), boundary_check=[1], padding_option='zero')
    v_0 = tl.cast(load_2, tl.float32)
    v_1 = acc + v_0
    v_2 = tl.full([], 0, tl.int32)
    v_3 = triton_helpers.maximum(v_2, v_1)
    v_4 = tl.cast(v_3, tl.float16)
    tl.store(tl.make_block_ptr(out, [1024, 1024], [1024, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_4, boundary_check=[0, 1])

def matmul(x: Tensor, y: Tensor, epilogue: Callable[[Tensor, tuple[Tensor, ...]], Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication of x and y with an optional epilogue function.
    Args:
        x (Tensor): Left matrix of shape [m, k].
        y (Tensor): Right matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        Tensor: Resulting matrix of shape [m, n].
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _BLOCK_SIZE_0 = 64
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_matmul, (triton.cdiv(1024, _BLOCK_SIZE_0) * triton.cdiv(1024, _BLOCK_SIZE_1),), x, y, epilogue.__closure__[0].cell_contents, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=2, num_stages=4)
    return out

--- assertExpectedJournal(TestExamples.test_template_via_closure2)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime import triton_helpers
from helion.runtime import default_launcher as _default_launcher

import test.test_examples as _global_source0

@triton.jit
def _helion_matmul(x, y, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    num_pid_m = tl.cdiv(1024, _BLOCK_SIZE_0)
    num_pid_n = tl.cdiv(1024, _BLOCK_SIZE_1)
    inner_2d_pid = tl.program_id(0)
    num_pid_in_group = 64 * num_pid_n
    group_id = inner_2d_pid // num_pid_in_group
    first_pid_m = group_id * 64
    group_size_m = min(num_pid_m - first_pid_m, 64)
    pid_0 = first_pid_m + inner_2d_pid % num_pid_in_group % group_size_m
    pid_1 = inner_2d_pid % num_pid_in_group // group_size_m
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    acc = tl.full([_BLOCK_SIZE_0, _BLOCK_SIZE_1], 0.0, tl.float32)
    for offset_2 in tl.range(0, 1024, _BLOCK_SIZE_2):
        acc_copy = acc
        acc_copy_0 = acc_copy
        load = tl.load(tl.make_block_ptr(x, [1024, 1024], [1024, 1], [offset_0, offset_2], [_BLOCK_SIZE_0, _BLOCK_SIZE_2], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        load_1 = tl.load(tl.make_block_ptr(y, [1024, 1024], [1024, 1], [offset_2, offset_1], [_BLOCK_SIZE_2, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
        acc = tl.dot(tl.cast(load, tl.float16), tl.cast(load_1, tl.float16), acc=acc_copy_0, input_precision='tf32', out_dtype=tl.float32)
    v_0 = tl.full([], 0, tl.int32)
    v_1 = triton_helpers.maximum(v_0, acc)
    v_2 = tl.cast(v_1, tl.float16)
    tl.store(tl.make_block_ptr(out, [1024, 1024], [1024, 1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_2, boundary_check=[0, 1])

def matmul(x: Tensor, y: Tensor, epilogue: Callable[[Tensor, tuple[Tensor, ...]], Tensor]=lambda acc, tile: acc, *, _launcher=_default_launcher):
    """
    Performs matrix multiplication of x and y with an optional epilogue function.
    Args:
        x (Tensor): Left matrix of shape [m, k].
        y (Tensor): Right matrix of shape [k, n].
        epilogue (Callable, optional): Function applied to the accumulator and tile indices
            after the matmul. Defaults to identity (no change).
    Returns:
        Tensor: Resulting matrix of shape [m, n].
    """
    m, k = x.size()
    k2, n = y.size()
    assert k == k2, f'size mismatch {k} != {k2}'
    out = torch.empty([m, n], dtype=torch.promote_types(x.dtype, y.dtype), device=x.device)
    _BLOCK_SIZE_0 = 64
    _BLOCK_SIZE_1 = 64
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_matmul, (triton.cdiv(1024, _BLOCK_SIZE_0) * triton.cdiv(1024, _BLOCK_SIZE_1),), x, y, out, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=2, num_stages=4)
    return out

--- assertExpectedJournal(TestExamples.test_welford)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_welford(x, weight, bias, out, bias_stride_0, out_stride_0, out_stride_1, weight_stride_0, x_stride_0, x_stride_1, m, n, eps, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < m
    acc_cnt = tl.full([_BLOCK_SIZE_0], 0, tl.float32)
    acc_mean = tl.full([_BLOCK_SIZE_0], 0, tl.float32)
    acc_m2 = tl.full([_BLOCK_SIZE_0], 0, tl.float32)
    for offset_1 in tl.range(0, n.to(tl.int32), _BLOCK_SIZE_1):
        indices_1 = offset_1 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_1 < n
        acc_mean_copy = acc_mean
        acc_cnt_copy = acc_cnt
        acc_m2_copy = acc_m2
        acc_mean_copy_0 = acc_mean_copy
        acc_cnt_copy_0 = acc_cnt_copy
        acc_m2_copy_0 = acc_m2_copy
        chunk = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_0[:, None] & mask_1[None, :], other=0)
        sum_x = tl.cast(tl.sum(chunk, 1), tl.float32)
        v_0 = chunk * chunk
        sum_x2 = tl.cast(tl.sum(v_0, 1), tl.float32)
        _BLOCK_SIZE_1_ = _BLOCK_SIZE_1
        v_1 = tl.cast(_BLOCK_SIZE_1_, tl.float32)
        v_2 = sum_x / v_1
        v_3 = sum_x * sum_x
        _BLOCK_SIZE_1__1 = _BLOCK_SIZE_1
        v_4 = tl.cast(_BLOCK_SIZE_1__1, tl.float32)
        v_5 = v_3 / v_4
        v_6 = sum_x2 - v_5
        v_7 = v_2 - acc_mean_copy_0
        _BLOCK_SIZE_1__2 = _BLOCK_SIZE_1
        v_8 = tl.cast(_BLOCK_SIZE_1__2, tl.float32)
        acc_cnt = acc_cnt_copy_0 + v_8
        v_10 = tl.full([], 1, tl.int32)
        v_11 = v_10 / acc_cnt
        _BLOCK_SIZE_1__3 = _BLOCK_SIZE_1
        v_12 = tl.cast(_BLOCK_SIZE_1__3, tl.float32)
        v_13 = v_11 * v_12
        v_14 = v_7 * v_13
        acc_mean = acc_mean_copy_0 + v_14
        v_16 = acc_m2_copy_0 + v_6
        v_17 = v_7 * v_7
        _BLOCK_SIZE_1__4 = _BLOCK_SIZE_1
        v_18 = tl.cast(_BLOCK_SIZE_1__4, tl.float32)
        v_19 = acc_cnt_copy_0 * v_18
        v_20 = v_19 / acc_cnt
        v_21 = v_17 * v_20
        acc_m2 = v_16 + v_21
    v_23 = acc_m2 / acc_cnt
    v_24 = v_23 + eps
    v_25 = libdevice.rsqrt(v_24)
    mean_col = acc_mean[:, None]
    rstd_col = v_25[:, None]
    for offset_2 in tl.range(0, n.to(tl.int32), _BLOCK_SIZE_2):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
        mask_2 = indices_2 < n
        mean_col_copy = mean_col
        rstd_col_copy = rstd_col
        mean_col_copy_0 = mean_col_copy
        rstd_col_copy_0 = rstd_col_copy
        xi_chuck = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_2[None, :] * x_stride_1), mask_0[:, None] & mask_2[None, :], other=0)
        load_1 = tl.load(weight + indices_2 * weight_stride_0, mask_2, other=0)
        w_chuck = load_1[None, :]
        load_2 = tl.load(bias + indices_2 * bias_stride_0, mask_2, other=0)
        b_chuck = load_2[None, :]
        v_26 = xi_chuck - mean_col_copy_0
        v_27 = v_26 * rstd_col_copy_0
        v_28 = v_27 * w_chuck
        v_29 = v_28 + b_chuck
        tl.store(out + (indices_0[:, None] * out_stride_0 + indices_2[None, :] * out_stride_1), v_29, mask_0[:, None] & mask_2[None, :])

def welford(weight: torch.Tensor, bias: torch.Tensor, x: torch.Tensor, eps: float=1e-05, *, _launcher=_default_launcher):
    """
    Applies LayerNorm using Welford's algorithm for mean/variance.
    Args:
        weight: weight tensor of shape [N]
        bias: bias tensor of shape [N]
        x: input tensor of shape [M, N]
    Returns:
        Output tensor of shape [M, N]
    """
    m, n = x.size()
    out = torch.empty([m, n], dtype=x.dtype, device=x.device)
    _BLOCK_SIZE_0 = 16
    _BLOCK_SIZE_1 = 16
    _BLOCK_SIZE_2 = 16
    _launcher(_helion_welford, (triton.cdiv(m, _BLOCK_SIZE_0),), x, weight, bias, out, bias.stride(0), out.stride(0), out.stride(1), weight.stride(0), x.stride(0), x.stride(1), m, n, eps, _BLOCK_SIZE_0, _BLOCK_SIZE_1, _BLOCK_SIZE_2, num_warps=4, num_stages=3)
    return out
