This file is automatically generated by assertExpectedJournal calls in test_views.py.
Update expected outputs by running tests with the EXPECTTEST_ACCEPT=1 environment variable set.

--- assertExpectedJournal(TestViews.test_reshape_sum)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_fn(x, out, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 3
    acc = tl.full([_BLOCK_SIZE_0], 0.0, tl.float32)
    for offset_3 in tl.range(0, 4, _BLOCK_SIZE_1):
        indices_3 = offset_3 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        for offset_4 in tl.range(0, 5, _BLOCK_SIZE_2):
            indices_4 = offset_4 + tl.arange(0, _BLOCK_SIZE_2).to(tl.int32)
            mask_2 = indices_4 < 5
            acc_copy = acc
            acc_copy_0 = acc_copy
            load = tl.load(x + (indices_0[:, None, None] * 20 + indices_3[None, :, None] * 5 + indices_4[None, None, :] * 1), mask_0[:, None, None] & mask_2[None, None, :], other=0)
            view = tl.reshape(load, [_BLOCK_SIZE_0, _BLOCK_SIZE_1 * _BLOCK_SIZE_2])
            sum_1 = tl.sum(view, 1)
            acc = acc_copy_0 + sum_1
    tl.store(out + indices_0 * 1, acc, mask_0)

def fn(x: torch.Tensor, *, _launcher=_default_launcher):
    out = x.new_empty([x.size(0)])
    _BLOCK_SIZE_0 = 4
    _BLOCK_SIZE_2 = 8
    _BLOCK_SIZE_1 = 4
    _RDIM_SIZE_3 = triton.next_power_of_2(_BLOCK_SIZE_1 * _BLOCK_SIZE_2)
    _launcher(_helion_fn, (triton.cdiv(3, _BLOCK_SIZE_0),), x, out, _BLOCK_SIZE_0, _BLOCK_SIZE_2, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestViews.test_softmax_unsqueeze)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    values = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), values, float('-inf'))
    amax = tl.max(_mask_to, 1)
    amax_1 = amax[:, None]
    v_0 = values - amax_1
    v_1 = v_0.to(tl.float32)
    v_2 = libdevice.exp(v_1)
    v_3 = v_2.to(tl.float16)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_3, 0)
    sum_1 = tl.sum(_mask_to_1, 1)
    sum_exp = sum_1[None, :]
    v_4 = v_3 / sum_exp
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_4, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_helion_softmax, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestViews.test_softmax_view_reshape)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from torch._inductor.runtime.triton_compat import libdevice
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_softmax(x, out, out_stride_0, out_stride_1, x_stride_0, x_stride_1, _m, _RDIM_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0
    indices_0 = offset_0 + tl.zeros([1], tl.int32)
    indices_1 = tl.arange(0, _RDIM_SIZE_1).to(tl.int32)
    mask_1 = indices_1 < _m
    values = tl.load(x + (indices_0[:, None] * x_stride_0 + indices_1[None, :] * x_stride_1), mask_1[None, :], other=0)
    _mask_to = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), values, float('-inf'))
    amax = tl.max(_mask_to, 1)
    amax_1 = tl.reshape(amax, [1, 1])
    v_0 = values - amax_1
    v_1 = v_0.to(tl.float32)
    v_2 = libdevice.exp(v_1)
    v_3 = v_2.to(tl.float16)
    _mask_to_1 = tl.where(tl.broadcast_to(mask_1[None, :], [1, _RDIM_SIZE_1]), v_3, 0)
    sum_1 = tl.sum(_mask_to_1, 1)
    sum_exp = tl.reshape(sum_1, [1, 1])
    v_4 = v_3 / sum_exp
    tl.store(out + (indices_0[:, None] * out_stride_0 + indices_1[None, :] * out_stride_1), v_4, mask_1[None, :])

def softmax(x: torch.Tensor, *, _launcher=_default_launcher):
    n, _m = x.size()
    out = torch.empty_like(x)
    _RDIM_SIZE_1 = triton.next_power_of_2(_m)
    _launcher(_helion_softmax, (n,), x, out, out.stride(0), out.stride(1), x.stride(0), x.stride(1), _m, _RDIM_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestViews.test_squeeze)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_fn(x, y, out, out_size_0, out_size_1, x_size_0, x_size_1, y_size_0, out_stride_0, out_stride_1, x_stride_0, x_stride_1, y_stride_0, y_stride_1, _BLOCK_SIZE_0: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    num_blocks_0 = tl.cdiv(x_size_0, _BLOCK_SIZE_0)
    pid_0 = tl.program_id(0) % num_blocks_0
    pid_1 = tl.program_id(0) // num_blocks_0
    offset_0 = pid_0 * _BLOCK_SIZE_0
    offset_1 = pid_1 * _BLOCK_SIZE_1
    load = tl.load(tl.make_block_ptr(x, [x_size_0, x_size_1], [x_stride_0, x_stride_1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), boundary_check=[0, 1], padding_option='zero')
    load_1 = tl.load(tl.make_block_ptr(y, [y_size_0, 1], [y_stride_0, y_stride_1], [offset_1, 0], [_BLOCK_SIZE_1, 1], [1, 0]), boundary_check=[0], padding_option='zero')
    squeeze = tl.reshape(load_1, [_BLOCK_SIZE_1])
    unsqueeze = squeeze[None, :]
    v_0 = load + unsqueeze
    tl.store(tl.make_block_ptr(out, [out_size_0, out_size_1], [out_stride_0, out_stride_1], [offset_0, offset_1], [_BLOCK_SIZE_0, _BLOCK_SIZE_1], [1, 0]), v_0, boundary_check=[0, 1])

def fn(x: torch.Tensor, y: torch.Tensor, *, _launcher=_default_launcher):
    out = torch.empty_like(x)
    _BLOCK_SIZE_0 = 32
    _BLOCK_SIZE_1 = 32
    _launcher(_helion_fn, (triton.cdiv(x.size(0), _BLOCK_SIZE_0) * triton.cdiv(x.size(1), _BLOCK_SIZE_1),), x, y, out, out.size(0), out.size(1), x.size(0), x.size(1), y.size(0), out.stride(0), out.stride(1), x.stride(0), x.stride(1), y.stride(0), y.stride(1), _BLOCK_SIZE_0, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return out

--- assertExpectedJournal(TestViews.test_stack_dim0)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_test_stack_dim0_kernel(a, b, c, result, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 65
    indices_3 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_3 < 3
    for offset_2 in tl.range(0, 129, _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_2 < 129
        a_tile = tl.load(a + (indices_0[:, None] * 129 + indices_2[None, :] * 1), mask_0[:, None] & mask_1[None, :], other=0)
        b_tile = tl.load(b + (indices_0[:, None] * 129 + indices_2[None, :] * 1), mask_0[:, None] & mask_1[None, :], other=0)
        c_tile = tl.load(c + (indices_0[:, None] * 129 + indices_2[None, :] * 1), mask_0[:, None] & mask_1[None, :], other=0)
        stack_idx = tl.arange(0, 4)
        broadcast_idx = stack_idx[:, None, None]
        expanded_0 = tl.expand_dims(a_tile, 0)
        expanded_1 = tl.expand_dims(b_tile, 0)
        expanded_2 = tl.expand_dims(c_tile, 0)
        stacked_result = tl.zeros_like(expanded_0)
        mask_3 = broadcast_idx == 0
        stacked_result = tl.where(mask_3, expanded_0, stacked_result)
        mask_4 = broadcast_idx == 1
        stacked_result = tl.where(mask_4, expanded_1, stacked_result)
        mask_5 = broadcast_idx == 2
        stacked_result = tl.where(mask_5, expanded_2, stacked_result)
        tl.store(result + (indices_3[:, None, None] * 8385 + indices_0[None, :, None] * 129 + indices_2[None, None, :] * 1), stacked_result, mask_2[:, None, None] & mask_0[None, :, None] & mask_1[None, None, :])

def test_stack_dim0_kernel(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, *, _launcher=_default_launcher):
    M, N = a.shape
    result = torch.zeros(3, M, N, dtype=a.dtype, device=a.device)
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_2 = 4
    _BLOCK_SIZE_1 = 32
    _launcher(_helion_test_stack_dim0_kernel, (triton.cdiv(65, _BLOCK_SIZE_0),), a, b, c, result, _BLOCK_SIZE_0, _RDIM_SIZE_2, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return result

--- assertExpectedJournal(TestViews.test_stack_non_power_of_2)
from __future__ import annotations

import torch
import triton
import triton.language as tl
from helion.runtime import default_launcher as _default_launcher

@triton.jit
def _helion_test_stack_non_power_of_2_kernel(a, b, c, result, _BLOCK_SIZE_0: tl.constexpr, _RDIM_SIZE_2: tl.constexpr, _BLOCK_SIZE_1: tl.constexpr):
    pid_0 = tl.program_id(0)
    offset_0 = pid_0 * _BLOCK_SIZE_0
    indices_0 = (offset_0 + tl.arange(0, _BLOCK_SIZE_0)).to(tl.int32)
    mask_0 = indices_0 < 65
    indices_3 = tl.arange(0, _RDIM_SIZE_2).to(tl.int32)
    mask_2 = indices_3 < 3
    for offset_2 in tl.range(0, 129, _BLOCK_SIZE_1):
        indices_2 = offset_2 + tl.arange(0, _BLOCK_SIZE_1).to(tl.int32)
        mask_1 = indices_2 < 129
        a_tile = tl.load(a + (indices_0[:, None] * 129 + indices_2[None, :] * 1), mask_0[:, None] & mask_1[None, :], other=0)
        b_tile = tl.load(b + (indices_0[:, None] * 129 + indices_2[None, :] * 1), mask_0[:, None] & mask_1[None, :], other=0)
        c_tile = tl.load(c + (indices_0[:, None] * 129 + indices_2[None, :] * 1), mask_0[:, None] & mask_1[None, :], other=0)
        stack_idx = tl.arange(0, 4)
        broadcast_idx = stack_idx[None, :, None]
        expanded_0 = tl.expand_dims(a_tile, 1)
        expanded_1 = tl.expand_dims(b_tile, 1)
        expanded_2 = tl.expand_dims(c_tile, 1)
        stacked_result = tl.zeros_like(expanded_0)
        mask_3 = broadcast_idx == 0
        stacked_result = tl.where(mask_3, expanded_0, stacked_result)
        mask_4 = broadcast_idx == 1
        stacked_result = tl.where(mask_4, expanded_1, stacked_result)
        mask_5 = broadcast_idx == 2
        stacked_result = tl.where(mask_5, expanded_2, stacked_result)
        tl.store(result + (indices_0[:, None, None] * 387 + indices_3[None, :, None] * 129 + indices_2[None, None, :] * 1), stacked_result, mask_0[:, None, None] & mask_2[None, :, None] & mask_1[None, None, :])

def test_stack_non_power_of_2_kernel(a: torch.Tensor, b: torch.Tensor, c: torch.Tensor, *, _launcher=_default_launcher):
    M, N = a.shape
    result = torch.zeros(M, 3, N, dtype=a.dtype, device=a.device)
    _BLOCK_SIZE_0 = 32
    _RDIM_SIZE_2 = 4
    _BLOCK_SIZE_1 = 32
    _launcher(_helion_test_stack_non_power_of_2_kernel, (triton.cdiv(65, _BLOCK_SIZE_0),), a, b, c, result, _BLOCK_SIZE_0, _RDIM_SIZE_2, _BLOCK_SIZE_1, num_warps=4, num_stages=3)
    return result
